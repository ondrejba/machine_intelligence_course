{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MI-MVI tutorial 3#\n",
    "\n",
    "Today we try to use recurrent neural networks (RNN) to predict words in english text.\n",
    "\n",
    " - Based on TF RNN tutorial: https://www.tensorflow.org/tutorials/recurrent\n",
    " - Pretty images an some thory come from: https://colah.github.io/posts/2015-08-Understanding-LSTMs/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A recurrent neural network can be thought of as multiple copies of the same network, each passing a message to a successor. Consider what happens if we unroll the loop:\n",
    "\n",
    "![RNN unroled](images/RNN-unrolled.png \"Structure of unrolled RNN.\")\n",
    "\n",
    "In the above diagram, a chunk of neural network, *A*, looks at some input *x_t* and outputs a value *h_t*. A loop allows information to be passed from one step of the network to the next. The *A* labelled boxes are elementary modules which vary with particular type of RNN. In standard (vanilla) RNN they look like:\n",
    "\n",
    "![RNN cenll](images/SimpleRNNcell.png)\n",
    "in comparison to LSTM cell:\n",
    "![LSTM cell](images/LSTMcell.png)\n",
    "\n",
    "We will focus on standard RNN cell. The yellow box with *tanh* is neural network layer. It combinates input *x_t* and previous state of the preceeding cell. We are going to build such cell from scratch, but you can implement the **tf.contrib.rnn.RNNCell** abstract class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Import** all packages that will be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, tarfile\n",
    "import collections\n",
    "from six.moves.urllib.request import urlretrieve\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download a [Penn Tree Bank (PTB)](https://catalog.ldc.upenn.edu/ldc99t42) dataset. We use an identical approach as in the last tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found and verified data/rnn/simple-examples.tgz\n"
     ]
    }
   ],
   "source": [
    "url = 'http://www.fit.vutbr.cz/~imikolov/rnnlm/'\n",
    "data_root = 'data/rnn'\n",
    "last_percent_reported = None\n",
    "\n",
    "# make sure the dataset directory exists\n",
    "if not os.path.isdir(data_root):\n",
    "  os.makedirs(data_root)\n",
    "\n",
    "def download_progress_hook(count, blockSize, totalSize):\n",
    "  \"\"\"A hook to report the progress of a download. This is mostly intended for users with\n",
    "  slow internet connections. Reports every 5% change in download progress.\n",
    "  \"\"\"\n",
    "  global last_percent_reported\n",
    "  percent = int(count * blockSize * 100 / totalSize)\n",
    "\n",
    "  if last_percent_reported != percent:\n",
    "    if percent % 5 == 0:\n",
    "      sys.stdout.write(\"%s%%\" % percent)\n",
    "      sys.stdout.flush()\n",
    "    else:\n",
    "      sys.stdout.write(\".\")\n",
    "      sys.stdout.flush()\n",
    "      \n",
    "    last_percent_reported = percent\n",
    "    \n",
    "def maybe_download(filename, expected_bytes, force=False):\n",
    "  \"\"\"Download a file if not present, and make sure it's the right size.\"\"\"\n",
    "  dest_filename = os.path.join(data_root, filename)\n",
    "  if force or not os.path.exists(dest_filename):\n",
    "    print('Attempting to download:', filename) \n",
    "    filename, _ = urlretrieve(url + filename, dest_filename, reporthook=download_progress_hook)\n",
    "    print('\\nDownload Complete!')\n",
    "  statinfo = os.stat(dest_filename)\n",
    "  if statinfo.st_size == expected_bytes:\n",
    "    print('Found and verified', dest_filename)\n",
    "  else:\n",
    "    raise Exception(\n",
    "      'Failed to verify ' + dest_filename + '. Can you get to it with a browser?')\n",
    "  return dest_filename\n",
    "\n",
    "train_filename = maybe_download('simple-examples.tgz', 34869662)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to unpack the downloaded data. The desired content is in **data** subdirectory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/rnn/simple-examples already present - Skipping extraction of data/rnn/simple-examples.tgz.\n",
      "['data/rnn/simple-examples/1-train', 'data/rnn/simple-examples/2-nbest-rescore', 'data/rnn/simple-examples/3-combination', 'data/rnn/simple-examples/4-data-generation', 'data/rnn/simple-examples/5-one-iter', 'data/rnn/simple-examples/6-recovery-during-training', 'data/rnn/simple-examples/7-dynamic-evaluation', 'data/rnn/simple-examples/8-direct', 'data/rnn/simple-examples/9-char-based-lm', 'data/rnn/simple-examples/data', 'data/rnn/simple-examples/models', 'data/rnn/simple-examples/rnnlm-0.2b', 'data/rnn/simple-examples/temp']\n"
     ]
    }
   ],
   "source": [
    "def maybe_extract(filename, force=False):\n",
    "  root = os.path.splitext(os.path.splitext(filename)[0])[0]  # remove .tar.gz\n",
    "  if os.path.isdir(root) and not force:\n",
    "    # You may override by setting force=True.\n",
    "    print('%s already present - Skipping extraction of %s.' % (root, filename))\n",
    "  else:\n",
    "    print('Extracting data for %s. This may take a while. Please wait.' % root)\n",
    "    tar = tarfile.open(filename)\n",
    "    sys.stdout.flush()\n",
    "    tar.extractall(data_root)\n",
    "    tar.close()\n",
    "  data_folders = [\n",
    "    os.path.join(root, d) for d in sorted(os.listdir(root))\n",
    "    if os.path.isdir(os.path.join(root, d))]\n",
    "  \n",
    "  print(data_folders)\n",
    "  return data_folders\n",
    "  \n",
    "train_folders = maybe_extract(train_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we define a few helpers to manipulate the data. Because a TF needs tensors of numbers we need to represent words by numbers. For this requirement we build a vocabulary from a given file. The vocabulary will contain a key-value pairs of following meaning 'word':ID.\n",
    "\n",
    "A **ptb_raw_data** method loads all necessary files, creates vocabularies and transform content of train, validation and test datafiles to number sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _read_words(filename):\n",
    "  with tf.gfile.GFile(filename, \"r\") as f:\n",
    "    return f.read().replace(\"\\n\", \"<eos>\").split()\n",
    "\n",
    "def _build_vocab(filename, wordsLimit=None):\n",
    "  data = _read_words(filename)\n",
    "  counter = collections.Counter(data)\n",
    "  count_pairs = sorted(counter.items(), key=lambda x: (-x[1], x[0]))\n",
    "  if (wordsLimit!=None):\n",
    "        count_pairs = count_pairs[0:wordsLimit]\n",
    "  words, _ = list(zip(*count_pairs))\n",
    "  word_to_id = dict(zip(words, range(len(words))))\n",
    "  return word_to_id\n",
    "\n",
    "def _file_to_word_ids(filename, word_to_id):\n",
    "  data = _read_words(filename)\n",
    "  return [word_to_id[word] for word in data if word in word_to_id]\n",
    "\n",
    "def ptb_raw_data(data_path=None, wordsLimit=None):\n",
    "  \"\"\"Load PTB raw data from data directory \"data_path\".\n",
    "  Reads PTB text files, converts strings to integer ids, and performs mini-batching of the inputs.\n",
    "  Args:\n",
    "    data_path: string path to the directory where simple-examples.tgz has been extracted.\n",
    "  Returns:\n",
    "    tuple (train_data, valid_data, test_data, vocabulary)\n",
    "    where each of the data objects can be passed to PTBIterator.\n",
    "  \"\"\"\n",
    "\n",
    "  train_path = os.path.join(data_path, \"ptb.train.txt\")\n",
    "  valid_path = os.path.join(data_path, \"ptb.valid.txt\")\n",
    "  test_path = os.path.join(data_path, \"ptb.test.txt\")\n",
    "\n",
    "  word_to_id = _build_vocab(train_path, wordsLimit)\n",
    "  train_data = _file_to_word_ids(train_path, word_to_id)\n",
    "  valid_data = _file_to_word_ids(valid_path, word_to_id)\n",
    "  test_data = _file_to_word_ids(test_path, word_to_id)\n",
    "  vocabulary = len(word_to_id)\n",
    "  return train_data, valid_data, test_data, vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us inspect the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train data len: 929589\n",
      "validation data len: 73760\n",
      "test data len: 82430\n",
      "vocabulary item count: 10000\n",
      "the first 30 vocabulary items: {'<unk>': 0, '30-year': 2495, \"'ll\": 963, '1930s': 4168, '1960s': 2058, \"'d\": 1182, '12-year': 3108, '52-week': 2059, '190.58-point': 2057, '20th': 4169, '1970s': 1748, '190-point': 1747, \"'m\": 964, '13th': 1059, '10-year': 2056, '#': 1181, '&': 72, '$': 14, '<eos>': 2, 'a': 6, '12-month': 4166, '1990s': 2494, \"'ve\": 670, '500-stock': 2496, \"'\": 131, '1980s': 1327, \"'re\": 234, '1920s': 4167, \"'s\": 9, 'N': 3}\n"
     ]
    }
   ],
   "source": [
    "# !!! TODO: pokud se ukaze, ze 10000 slov ve slovniku je moc na one hot encoding, profiltrujem slovnik.\n",
    "wordsLimit=10000\n",
    "\n",
    "# !!! TODO: posefit one hot encoding\n",
    "\n",
    "train_data, valid_data, test_data, vocabulary = ptb_raw_data(os.path.join(data_root, 'simple-examples/data'), wordsLimit)\n",
    "vocab = _build_vocab(os.path.join(data_root, 'simple-examples','data','ptb.test.txt'), wordsLimit)\n",
    "firstitems = {k: vocab[k] for k in sorted(vocab.keys())[:30]}\n",
    "\n",
    "print('train data len:', len(train_data))\n",
    "print('validation data len:', len(valid_data))\n",
    "print('test data len:', len(test_data))\n",
    "print('vocabulary item count:', vocabulary)\n",
    "print('the first 30 vocabulary items:', firstitems)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ptb_producer(raw_data, batch_size, num_steps, name=None):\n",
    "  \"\"\"Iterate on the raw PTB data.\n",
    "  This chunks up raw_data into batches of examples and returns Tensors that are drawn from these batches.\n",
    "  Args:\n",
    "    raw_data: one of the raw data outputs from ptb_raw_data.\n",
    "    batch_size: int, the batch size.\n",
    "    num_steps: int, the number of unrolls.\n",
    "    name: the name of this operation (optional).\n",
    "  Returns:\n",
    "    A pair of Tensors, each shaped [batch_size, num_steps]. The second element\n",
    "    of the tuple is the same data time-shifted to the right by one.\n",
    "  Raises:\n",
    "    tf.errors.InvalidArgumentError: if batch_size or num_steps are too high.\n",
    "  \"\"\"\n",
    "  with tf.name_scope(name, \"PTBProducer\", [raw_data, batch_size, num_steps]):\n",
    "    raw_data = tf.convert_to_tensor(raw_data, name=\"raw_data\", dtype=tf.int32)\n",
    "\n",
    "    data_len = tf.size(raw_data)\n",
    "    batch_len = data_len // batch_size\n",
    "    data = tf.reshape(raw_data[0 : batch_size * batch_len], [batch_size, batch_len])\n",
    "\n",
    "    epoch_size = (batch_len - 1) // num_steps\n",
    "    assertion = tf.assert_positive(epoch_size, message=\"epoch_size == 0, decrease batch_size or num_steps\")\n",
    "    with tf.control_dependencies([assertion]):\n",
    "      epoch_size = tf.identity(epoch_size, name=\"epoch_size\")\n",
    "\n",
    "    i = tf.train.range_input_producer(epoch_size, shuffle=False).dequeue()\n",
    "    x = tf.strided_slice(data, [0, i * num_steps], [batch_size, (i + 1) * num_steps])\n",
    "    x.set_shape([batch_size, num_steps])\n",
    "    y = tf.strided_slice(data, [0, i * num_steps + 1],[batch_size, (i + 1) * num_steps + 1])\n",
    "    y.set_shape([batch_size, num_steps])\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is a time to put all together... "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vanilla RNN and LSTM ##\n",
    "\n",
    "Preparing the building blocks for both types of RNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_size = 200\n",
    "init_scale = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RNN_logits(states, output_size):\n",
    "        \n",
    "    # RNN parameters\n",
    "    V = tf.get_variable('V', shape=[state_size, output_size], \n",
    "                        initializer=tf.random_uniform_initializer(minval=-init_scale, maxval=init_scale))\n",
    "    bo = tf.get_variable('bl', shape=[output_size], initializer=tf.constant_initializer(0.))\n",
    "    \n",
    "    # calculate logits\n",
    "    return tf.matmul(states, V) + bo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![RNN cenll](images/SimpleRNNcell.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RNN_step(previous_hidden_state, input_tensor):\n",
    "    \n",
    "    # RNN parameters\n",
    "    W = tf.get_variable(\"W\", shape=[state_size, state_size], \n",
    "                        initializer=tf.random_uniform_initializer(minval=-init_scale, maxval=init_scale))\n",
    "    U = tf.get_variable(\"U\", shape=[state_size, state_size], \n",
    "                        initializer=tf.random_uniform_initializer(minval=-init_scale, maxval=init_scale))\n",
    "    b = tf.get_variable(\"b\", shape=[state_size], initializer=tf.constant_initializer(0.))\n",
    "    \n",
    "    # calculate new hidden state\n",
    "    hidden_state = tf.tanh(tf.matmul(previous_hidden_state, W) + tf.matmul(input_tensor,U) + b)\n",
    "    \n",
    "    return hidden_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![LSTM cell](images/LSTMcell.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LSTM_step(previous_hidden_state, input_tensor):\n",
    "    \n",
    "    # weights for input\n",
    "    W = tf.get_variable('W', shape=[4, state_size, state_size], \n",
    "                        initializer=tf.random_uniform_initializer(minval=-init_scale, maxval=init_scale))\n",
    "    # weights for previous hidden state\n",
    "    U = tf.get_variable('U', shape=[4, state_size, state_size], \n",
    "                        initializer=tf.random_uniform_initializer(minval=-init_scale, maxval=init_scale))\n",
    "    \n",
    "    bi = tf.get_variable(\"bi\", shape=[state_size], initializer=tf.constant_initializer(0.))\n",
    "    bf = tf.get_variable(\"bf\", shape=[state_size], initializer=tf.constant_initializer(0.))\n",
    "    bo = tf.get_variable(\"bo\", shape=[state_size], initializer=tf.constant_initializer(0.))\n",
    "    bc = tf.get_variable(\"bc\", shape=[state_size], initializer=tf.constant_initializer(0.))\n",
    "    \n",
    "    # gather previous internal state and output state\n",
    "    state, cell = tf.unstack(previous_hidden_state)\n",
    "    \n",
    "    # gates\n",
    "    input_gate = tf.sigmoid(tf.matmul(input_tensor, U[0]) + tf.matmul(state, W[0]) + bi)\n",
    "    forget_gate = tf.sigmoid(tf.matmul(input_tensor, U[1]) + tf.matmul(state, W[1]) + bf)\n",
    "    output_gate = tf.sigmoid(tf.matmul(input_tensor, U[2]) + tf.matmul(state, W[2]) + bo)\n",
    "    gate_weights = tf.tanh(tf.matmul(input_tensor, U[3]) + tf.matmul(state, W[3]) + bc)\n",
    "    \n",
    "    # new internal cell state\n",
    "    cell = cell * forget_gate + gate_weights * input_gate\n",
    "    \n",
    "    # output state\n",
    "    state = tf.tanh(cell) * output_gate\n",
    "    return tf.stack([state, cell])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets try to understand **tf.transpose** by playing with it for a while:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[ 1  2  3]\n",
      "  [ 7  8  9]]\n",
      "\n",
      " [[ 4  5  6]\n",
      "  [10 11 12]]]\n"
     ]
    }
   ],
   "source": [
    "c = tf.constant([[[ 1,  2,  3],\n",
    "                  [ 4,  5,  6]],\n",
    "                 [[ 7,  8,  9],\n",
    "                  [10, 11, 12]]])\n",
    "\n",
    "ctr = tf.transpose(c, perm=[1, 0, 2])\n",
    "\n",
    "with tf.Session() as session:\n",
    "  session.run(ctr)\n",
    "  print(ctr.eval())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vanilla RNN #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = vocabulary\n",
    "num_steps = 20\n",
    "batch_size = 20\n",
    "max_gradient_norm = 5\n",
    "learning_rate = 1.0\n",
    "\n",
    "# basic RNN  cell\n",
    "rnn_type = \"vanilla\"\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# take a subset of data\n",
    "input_tensor, labels_tensor = ptb_producer(train_data, batch_size=batch_size, num_steps=num_steps)\n",
    "\n",
    "# TODO: kde se naplni to embeddings?\n",
    "embeddings = tf.get_variable(\"embeddings\", [num_classes, state_size])\n",
    "# kdyz se tady z nej maji vybrat hodnoty (sloupce nebo radky?) podle idcek v input_tensor\n",
    "rnn_inputs = tf.nn.embedding_lookup(embeddings, input_tensor)\n",
    "\n",
    "# an initial hidden state zero-filled tensor\n",
    "init_hidden_state = tf.cast(np.zeros([batch_size, state_size]), tf.float32)\n",
    "\n",
    "# TODO: proc se to transponuje tam a zase zpatky?\n",
    "states = tf.scan(RNN_step, tf.transpose(rnn_inputs, [1,0,2]), initializer=init_hidden_state) \n",
    "states = tf.transpose(states, [1,0,2])\n",
    "\n",
    "# TODO: proc se otaci poradi? a hned dvakrat?\n",
    "states_reshaped = tf.reshape(states, [-1, state_size])\n",
    "logits = RNN_logits(states_reshaped, num_classes)\n",
    "logits = tf.reshape(logits, [batch_size, num_steps, -1])\n",
    "\n",
    "predictions = tf.nn.softmax(logits)\n",
    "\n",
    "# calculate a difference between predited and correct labels\n",
    "losses = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=labels_tensor)\n",
    "loss = tf.reduce_mean(losses)\n",
    "\n",
    "trainable_vars = tf.trainable_variables()\n",
    "grads, _ = tf.clip_by_global_norm(tf.gradients(loss, trainable_vars), max_gradient_norm)\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "train_op = optimizer.apply_gradients(zip(grads, trainable_vars), \n",
    "                                     global_step=tf.contrib.framework.get_or_create_global_step())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = vocabulary\n",
    "num_steps = 20\n",
    "batch_size = 20\n",
    "max_gradient_norm = 5\n",
    "learning_rate = 1.0\n",
    "\n",
    "#LSTM cell\n",
    "rnn_type = \"LSTM\"\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# take a subset of data\n",
    "input_tensor, labels_tensor = ptb_producer(train_data, batch_size=batch_size, num_steps=num_steps)\n",
    "\n",
    "# TODO: kde se naplni to embeddings?\n",
    "embeddings = tf.get_variable(\"embeddings\", [num_classes, state_size])\n",
    "# kdyz se tady z nej maji vybrat hodnoty (sloupce nebo radky?) podle idcek v input_tensor\n",
    "rnn_inputs = tf.nn.embedding_lookup(embeddings, input_tensor)\n",
    "\n",
    "# an initial hidden state zero-filled tensor\n",
    "init_hidden_state = tf.cast(np.zeros([2, batch_size, state_size]), dtype=tf.float32)\n",
    "\n",
    "# TODO: proc se to transponuje tam a zase zpatky?\n",
    "states = tf.scan(LSTM_step, tf.transpose(rnn_inputs, [1,0,2]), initializer=init_hidden_state) \n",
    "states = tf.transpose(states, [1,2,0,3])[0]\n",
    "\n",
    "# TODO: proc se otaci poradi? a hned dvakrat?\n",
    "states_reshaped = tf.reshape(states, [-1, state_size])\n",
    "logits = RNN_logits(states_reshaped, num_classes)\n",
    "logits = tf.reshape(logits, [batch_size, num_steps, -1])\n",
    "\n",
    "predictions = tf.nn.softmax(logits)\n",
    "\n",
    "# calculate a difference between predited and correct labels\n",
    "losses = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=labels_tensor)\n",
    "loss = tf.reduce_mean(losses)\n",
    "\n",
    "trainable_vars = tf.trainable_variables()\n",
    "grads, _ = tf.clip_by_global_norm(tf.gradients(loss, trainable_vars), max_gradient_norm)\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "train_op = optimizer.apply_gradients(zip(grads, trainable_vars), \n",
    "                                     global_step=tf.contrib.framework.get_or_create_global_step())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RNN type:  LSTM\n",
      "step: 0\n",
      "loss: [9.2103758, None]\n",
      "\n",
      "step: 1\n",
      "loss: [9.1972437, None]\n",
      "\n",
      "step: 2\n",
      "loss: [9.1801109, None]\n",
      "\n",
      "step: 3\n",
      "loss: [9.1664448, None]\n",
      "\n",
      "step: 4\n",
      "loss: [9.1478958, None]\n",
      "\n",
      "step: 5\n",
      "loss: [9.1334858, None]\n",
      "\n",
      "step: 6\n",
      "loss: [9.120285, None]\n",
      "\n",
      "step: 7\n",
      "loss: [9.1069813, None]\n",
      "\n",
      "step: 8\n",
      "loss: [9.0864716, None]\n",
      "\n",
      "step: 9\n",
      "loss: [9.0761538, None]\n",
      "\n",
      "step: 10\n",
      "loss: [9.0587387, None]\n",
      "\n",
      "step: 11\n",
      "loss: [9.0128689, None]\n",
      "\n",
      "step: 12\n",
      "loss: [9.0143213, None]\n",
      "\n",
      "step: 13\n",
      "loss: [9.0182877, None]\n",
      "\n",
      "step: 14\n",
      "loss: [8.9917889, None]\n",
      "\n",
      "step: 15\n",
      "loss: [8.9834356, None]\n",
      "\n",
      "step: 16\n",
      "loss: [8.9227142, None]\n",
      "\n",
      "step: 17\n",
      "loss: [8.9316635, None]\n",
      "\n",
      "step: 18\n",
      "loss: [8.879118, None]\n",
      "\n",
      "step: 19\n",
      "loss: [8.9076252, None]\n",
      "\n",
      "step: 20\n",
      "loss: [8.8004942, None]\n",
      "\n",
      "step: 21\n",
      "loss: [8.7474966, None]\n",
      "\n",
      "step: 22\n",
      "loss: [8.6039553, None]\n",
      "\n",
      "step: 23\n",
      "loss: [8.5174246, None]\n",
      "\n",
      "step: 24\n",
      "loss: [8.3974323, None]\n",
      "\n",
      "step: 25\n",
      "loss: [8.3970432, None]\n",
      "\n",
      "step: 26\n",
      "loss: [7.9700203, None]\n",
      "\n",
      "step: 27\n",
      "loss: [7.8034649, None]\n",
      "\n",
      "step: 28\n",
      "loss: [7.8849301, None]\n",
      "\n",
      "step: 29\n",
      "loss: [7.6538215, None]\n",
      "\n",
      "step: 30\n",
      "loss: [7.7518096, None]\n",
      "\n",
      "step: 31\n",
      "loss: [7.7790909, None]\n",
      "\n",
      "step: 32\n",
      "loss: [7.4587102, None]\n",
      "\n",
      "step: 33\n",
      "loss: [7.6706734, None]\n",
      "\n",
      "step: 34\n",
      "loss: [7.4594808, None]\n",
      "\n",
      "step: 35\n",
      "loss: [7.5197563, None]\n",
      "\n",
      "step: 36\n",
      "loss: [7.5798664, None]\n",
      "\n",
      "step: 37\n",
      "loss: [7.3403888, None]\n",
      "\n",
      "step: 38\n",
      "loss: [7.5976205, None]\n",
      "\n",
      "step: 39\n",
      "loss: [7.5284538, None]\n",
      "\n",
      "step: 40\n",
      "loss: [7.3220453, None]\n",
      "\n",
      "step: 41\n",
      "loss: [7.1718488, None]\n",
      "\n",
      "step: 42\n",
      "loss: [7.5610228, None]\n",
      "\n",
      "step: 43\n",
      "loss: [7.4297023, None]\n",
      "\n",
      "step: 44\n",
      "loss: [7.50108, None]\n",
      "\n",
      "step: 45\n",
      "loss: [7.3212628, None]\n",
      "\n",
      "step: 46\n",
      "loss: [7.2456756, None]\n",
      "\n",
      "step: 47\n",
      "loss: [7.4779477, None]\n",
      "\n",
      "step: 48\n",
      "loss: [7.4048433, None]\n",
      "\n",
      "step: 49\n",
      "loss: [7.1849117, None]\n",
      "\n",
      "step: 50\n",
      "loss: [7.0749183, None]\n",
      "\n",
      "step: 51\n",
      "loss: [7.2575927, None]\n",
      "\n",
      "step: 52\n",
      "loss: [7.2492347, None]\n",
      "\n",
      "step: 53\n",
      "loss: [7.2410269, None]\n",
      "\n",
      "step: 54\n",
      "loss: [7.131506, None]\n",
      "\n",
      "step: 55\n",
      "loss: [7.2090063, None]\n",
      "\n",
      "step: 56\n",
      "loss: [7.3464375, None]\n",
      "\n",
      "step: 57\n",
      "loss: [6.8413944, None]\n",
      "\n",
      "step: 58\n",
      "loss: [6.9679642, None]\n",
      "\n",
      "step: 59\n",
      "loss: [7.2306008, None]\n",
      "\n",
      "step: 60\n",
      "loss: [7.0797005, None]\n",
      "\n",
      "step: 61\n",
      "loss: [7.1468349, None]\n",
      "\n",
      "step: 62\n",
      "loss: [7.1853781, None]\n",
      "\n",
      "step: 63\n",
      "loss: [7.2875538, None]\n",
      "\n",
      "step: 64\n",
      "loss: [7.1472578, None]\n",
      "\n",
      "step: 65\n",
      "loss: [7.0361199, None]\n",
      "\n",
      "step: 66\n",
      "loss: [7.2123594, None]\n",
      "\n",
      "step: 67\n",
      "loss: [6.7951503, None]\n",
      "\n",
      "step: 68\n",
      "loss: [7.4734344, None]\n",
      "\n",
      "step: 69\n",
      "loss: [7.6452494, None]\n",
      "\n",
      "step: 70\n",
      "loss: [7.3906903, None]\n",
      "\n",
      "step: 71\n",
      "loss: [7.3862295, None]\n",
      "\n",
      "step: 72\n",
      "loss: [7.1264081, None]\n",
      "\n",
      "step: 73\n",
      "loss: [7.1467605, None]\n",
      "\n",
      "step: 74\n",
      "loss: [7.1377068, None]\n",
      "\n",
      "step: 75\n",
      "loss: [6.9228368, None]\n",
      "\n",
      "step: 76\n",
      "loss: [7.0418673, None]\n",
      "\n",
      "step: 77\n",
      "loss: [7.1638613, None]\n",
      "\n",
      "step: 78\n",
      "loss: [7.0503531, None]\n",
      "\n",
      "step: 79\n",
      "loss: [6.9773917, None]\n",
      "\n",
      "step: 80\n",
      "loss: [6.961812, None]\n",
      "\n",
      "step: 81\n",
      "loss: [6.9836063, None]\n",
      "\n",
      "step: 82\n",
      "loss: [6.8925066, None]\n",
      "\n",
      "step: 83\n",
      "loss: [6.9702401, None]\n",
      "\n",
      "step: 84\n",
      "loss: [7.0577016, None]\n",
      "\n",
      "step: 85\n",
      "loss: [7.0035772, None]\n",
      "\n",
      "step: 86\n",
      "loss: [7.0881982, None]\n",
      "\n",
      "step: 87\n",
      "loss: [6.9729786, None]\n",
      "\n",
      "step: 88\n",
      "loss: [6.9387884, None]\n",
      "\n",
      "step: 89\n",
      "loss: [6.8659377, None]\n",
      "\n",
      "step: 90\n",
      "loss: [6.9335184, None]\n",
      "\n",
      "step: 91\n",
      "loss: [7.0163226, None]\n",
      "\n",
      "step: 92\n",
      "loss: [6.8845301, None]\n",
      "\n",
      "step: 93\n",
      "loss: [7.1584444, None]\n",
      "\n",
      "step: 94\n",
      "loss: [7.1673923, None]\n",
      "\n",
      "step: 95\n",
      "loss: [7.1092339, None]\n",
      "\n",
      "step: 96\n",
      "loss: [7.1533065, None]\n",
      "\n",
      "step: 97\n",
      "loss: [6.8556523, None]\n",
      "\n",
      "step: 98\n",
      "loss: [6.7205296, None]\n",
      "\n",
      "step: 99\n",
      "loss: [7.0305352, None]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "num_training_steps = 100\n",
    "\n",
    "num_classes = vocabulary\n",
    "num_steps = 20\n",
    "batch_size = 20\n",
    "num_layers = 2\n",
    "\n",
    "# TODO: v poslednim kroku ten ptb_producer neco dela spatne:\n",
    "# ERROR:tensorflow:Exception in QueueRunner: Enqueue operation was cancelled\n",
    "# hrabe s konstantni delkou kroku uz mimo rozsah dat? \n",
    "# tedy posledni vyber by mel byt neco v duchu:\n",
    "# x = tf.strided_slice(data, [0, i * num_steps], [batch_size, max(size(data)[spravna_dim], (i + 1) * num_steps]))\n",
    "\n",
    "# navic mam podle vypisu input a labels pocit, ze ptb_prodecer nevraci labely jako data \n",
    "# shiftnuta o 1 vpravo, jak se tvrdi v popisku; kdyz jsem si je nechal vypsat, nevypadaji \n",
    "# vubec, ze by se prekryvala :-(\n",
    "\n",
    "with tf.Session() as session:\n",
    "  print(\"RNN type: \", rnn_type)\n",
    "  session.run(tf.global_variables_initializer())\n",
    "    \n",
    "  #\n",
    "  input_coord = tf.train.Coordinator() \n",
    "  input_threads = tf.train.start_queue_runners(session, coord=input_coord)\n",
    "  #\n",
    "    \n",
    "  for step in range(num_training_steps):\n",
    "    \n",
    "    loss_val = session.run([loss, train_op], feed_dict={})\n",
    "    \n",
    "    \n",
    "    input_vals, labels_vals = session.run([input_tensor, labels_tensor])\n",
    "    \n",
    "    #print()\n",
    "    #print(input_vals)\n",
    "    #print(labels_vals)\n",
    "    #print()\n",
    "    \n",
    "    \n",
    "    #print(\"input:\", input_tensor.eval())\n",
    "    #print(\"labels:\", labels_tensor.eval())\n",
    "    #print(\"embeddings:\", embeddings.eval())\n",
    "    #print(\"rnn_inputs:\", rnn_inputs.eval())\n",
    "    print(\"step:\", step)\n",
    "    print(\"loss:\", loss_val)\n",
    "    print()\n",
    "    \n",
    "  #  \n",
    "  input_coord.request_stop()\n",
    "  input_coord.join(input_threads)  \n",
    "  #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO:\n",
    "something like: what you trained above is basically the whole model, but we'll need to make changes that are beyond the scope of this tutorial to actually make it work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-layer LSTM ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# configuration\n",
    "num_classes = vocabulary\n",
    "max_gradient_norm = 5\n",
    "hidden_size = 200\n",
    "num_steps = 20\n",
    "batch_size = 20\n",
    "num_layers = 2\n",
    "\n",
    "learning_rate = 1.0\n",
    "learning_rate_decay = 0.5\n",
    "epoch_end_decay = 4\n",
    "\n",
    "num_epochs = 13\n",
    "\n",
    "# LSTM cell\n",
    "rnn_type = \"LSTM\"\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# take a subset of data\n",
    "input_tensor, labels_tensor = ptb_producer(train_data, batch_size=batch_size, num_steps=num_steps)\n",
    "\n",
    "# TODO: kde se naplni to embeddings?\n",
    "embeddings = tf.get_variable(\"embeddings\", [num_classes, state_size])\n",
    "# kdyz se tady z nej maji vybrat hodnoty (sloupce nebo radky?) podle idcek v input_tensor\n",
    "rnn_inputs = tf.nn.embedding_lookup(embeddings, input_tensor)\n",
    "\n",
    "def build_layer(rnn_inputs, layer_idx):\n",
    "    \n",
    "    with tf.variable_scope(\"layer{}\".format(layer_idx)):\n",
    "    \n",
    "        # truncated backprop\n",
    "        hidden_state = tf.placeholder(tf.float32, shape=[2, batch_size, state_size])\n",
    "        \n",
    "        # TODO: proc se to transponuje tam a zase zpatky?\n",
    "        states = tf.scan(LSTM_step, tf.transpose(rnn_inputs, [1,0,2]), initializer=hidden_state) \n",
    "        states = tf.transpose(states, [1,2,0,3])\n",
    "        \n",
    "        return states, hidden_state\n",
    "    \n",
    "sequence = rnn_inputs\n",
    "\n",
    "final_states = []\n",
    "hidden_states = []\n",
    "\n",
    "for layer_idx in range(num_layers):\n",
    "    states, hidden_state = build_layer(sequence, layer_idx)\n",
    "    final_states.append(states[:, :, -1, :])\n",
    "    hidden_states.append(hidden_state)\n",
    "    \n",
    "    sequence = states[0]\n",
    "    \n",
    "states_reshaped = tf.reshape(sequence, [-1, state_size])\n",
    "logits = RNN_logits(states_reshaped, num_classes)\n",
    "logits = tf.reshape(logits, [batch_size, num_steps, -1])\n",
    "\n",
    "predictions = tf.nn.softmax(logits)\n",
    "\n",
    "# calculate a difference between predited and correct labels\n",
    "losses = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=labels_tensor)\n",
    "loss = tf.reduce_sum(losses) / batch_size\n",
    "\n",
    "learning_rate_tensor = tf.Variable(learning_rate, name=\"learning_rate\")\n",
    "learning_rate_pl = tf.placeholder(tf.float32, name=\"learning_rate_pl\")\n",
    "assign_learning_rate = tf.assign(learning_rate_tensor, learning_rate_pl)\n",
    "\n",
    "trainable_vars = tf.trainable_variables()\n",
    "grads, _ = tf.clip_by_global_norm(tf.gradients(loss, trainable_vars), max_gradient_norm)\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate_tensor)\n",
    "train_op = optimizer.apply_gradients(zip(grads, trainable_vars), \n",
    "                                     global_step=tf.contrib.framework.get_or_create_global_step())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "epoch_size = ((len(train_data) // batch_size) - 1) // num_steps\n",
    "\n",
    "with tf.Session() as session:\n",
    "    \n",
    "  print(\"RNN type: \", rnn_type)\n",
    "  print()\n",
    "    \n",
    "  saver = tf.train.Saver()\n",
    "\n",
    "  session.run(tf.global_variables_initializer())\n",
    "    \n",
    "  #\n",
    "  input_coord = tf.train.Coordinator() \n",
    "  input_threads = tf.train.start_queue_runners(session, coord=input_coord)\n",
    "  #\n",
    "    \n",
    "  for epoch in range(num_epochs):\n",
    "      \n",
    "      learning_rate_decay = learning_rate_decay ** max(epoch + 1 - epoch_end_decay, 0.0)\n",
    "      session.run(assign_learning_rate, feed_dict={\n",
    "          learning_rate_pl: learning_rate * learning_rate_decay\n",
    "      })\n",
    "        \n",
    "      total_loss = 0\n",
    "      total_time_steps = 0\n",
    "   \n",
    "      epoch_hidden_states = []\n",
    "      for state_pl in hidden_states:\n",
    "            epoch_hidden_states.append(np.zeros((2, batch_size, state_size)))\n",
    "\n",
    "      for step in range(epoch_size):\n",
    "\n",
    "        # build feed dict\n",
    "        feed_dict = {}\n",
    "        \n",
    "        for state_pl, state_val in zip(hidden_states, epoch_hidden_states):\n",
    "            feed_dict[state_pl] = state_val\n",
    "            \n",
    "        loss_val, _, epoch_hidden_states = session.run([loss, train_op, final_states], feed_dict=feed_dict)\n",
    "\n",
    "        total_loss += loss_val\n",
    "        total_time_steps += num_steps\n",
    "            \n",
    "      epoch_perplexity = np.exp(total_loss / total_time_steps)\n",
    "    \n",
    "      print(\"epoch {} - perplexity: {:.3f}\".format(epoch + 1, epoch_perplexity))\n",
    "    \n",
    "  saver.save(session, \"language-rnn\", global_step=0)\n",
    "    \n",
    "  #  \n",
    "  input_coord.request_stop()\n",
    "  input_coord.join(input_threads)  \n",
    "  #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "with Xavier init:\n",
    "epoch 1 - perplexity: 323.895\n",
    "epoch 2 - perplexity: 159.690\n",
    "epoch 3 - perplexity: 124.899\n",
    "epoch 4 - perplexity: 106.407\n",
    "epoch 5 - perplexity: 94.154\n",
    "epoch 6 - perplexity: 85.495\n",
    "epoch 7 - perplexity: 79.216\n",
    "epoch 8 - perplexity: 74.287\n",
    "epoch 9 - perplexity: 70.246\n",
    "epoch 10 - perplexity: 67.009\n",
    "epoch 11 - perplexity: 64.202\n",
    "epoch 12 - perplexity: 61.849\n",
    "epoch 13 - perplexity: 59.895\n",
    "\n",
    "with uniform init:\n",
    "epoch 1 - perplexity: 297.000\n",
    "epoch 2 - perplexity: 151.629\n",
    "epoch 3 - perplexity: 119.196\n",
    "epoch 4 - perplexity: 101.152\n",
    "epoch 5 - perplexity: 89.644\n",
    "epoch 6 - perplexity: 81.587\n",
    "epoch 7 - perplexity: 75.741\n",
    "epoch 8 - perplexity: 71.162\n",
    "epoch 9 - perplexity: 67.542\n",
    "epoch 10 - perplexity: 64.574\n",
    "epoch 11 - perplexity: 61.933\n",
    "epoch 12 - perplexity: 59.742\n",
    "epoch 13 - perplexity: 57.869\n",
    "\n",
    "with truncated backprop 1:\n",
    "epoch 1 - perplexity: 300.770\n",
    "epoch 2 - perplexity: 153.738\n",
    "epoch 3 - perplexity: 121.265\n",
    "epoch 4 - perplexity: 103.269\n",
    "epoch 5 - perplexity: 91.628\n",
    "epoch 6 - perplexity: 83.777\n",
    "epoch 7 - perplexity: 77.884\n",
    "epoch 8 - perplexity: 73.411\n",
    "epoch 9 - perplexity: 69.867\n",
    "epoch 10 - perplexity: 66.884\n",
    "epoch 11 - perplexity: 64.256\n",
    "epoch 12 - perplexity: 61.849\n",
    "epoch 13 - perplexity: 60.092\n",
    "\n",
    "with truncated backprop 2:\n",
    "epoch 1 - perplexity: 276.147\n",
    "epoch 2 - perplexity: 134.308\n",
    "epoch 3 - perplexity: 102.976\n",
    "epoch 4 - perplexity: 86.773\n",
    "epoch 5 - perplexity: 76.711\n",
    "epoch 6 - perplexity: 69.913\n",
    "epoch 7 - perplexity: 65.111\n",
    "epoch 8 - perplexity: 61.424\n",
    "epoch 9 - perplexity: 58.371\n",
    "epoch 10 - perplexity: 55.948\n",
    "epoch 11 - perplexity: 53.728\n",
    "epoch 12 - perplexity: 51.879\n",
    "epoch 13 - perplexity: 50.305"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 20\n",
    "\n",
    "rnn_type = \"LSTM\"\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# take a subset of data\n",
    "input_tensor, labels_tensor = ptb_producer(train_data, batch_size=batch_size, num_steps=num_steps)\n",
    "\n",
    "# TODO: kde se naplni to embeddings?\n",
    "embeddings = tf.get_variable(\"embeddings\", [num_classes, state_size])\n",
    "# kdyz se tady z nej maji vybrat hodnoty (sloupce nebo radky?) podle idcek v input_tensor\n",
    "rnn_inputs = tf.nn.embedding_lookup(embeddings, input_tensor)\n",
    "\n",
    "def build_layer(rnn_inputs, layer_idx):\n",
    "    \n",
    "    with tf.variable_scope(\"layer{}\".format(layer_idx)):\n",
    "    \n",
    "        # truncated backprop\n",
    "        hidden_state = tf.placeholder(tf.float32, shape=[2, batch_size, state_size])\n",
    "        \n",
    "        # TODO: proc se to transponuje tam a zase zpatky?\n",
    "        states = tf.scan(LSTM_step, tf.transpose(rnn_inputs, [1,0,2]), initializer=hidden_state) \n",
    "        states = tf.transpose(states, [1,2,0,3])\n",
    "        \n",
    "        return states, hidden_state\n",
    "    \n",
    "sequence = rnn_inputs\n",
    "\n",
    "final_states = []\n",
    "hidden_states = []\n",
    "\n",
    "for layer_idx in range(num_layers):\n",
    "    states, hidden_state = build_layer(sequence, layer_idx)\n",
    "    final_states.append(states[:, :, -1, :])\n",
    "    hidden_states.append(hidden_state)\n",
    "    \n",
    "    sequence = states[0]\n",
    "    \n",
    "states_reshaped = tf.reshape(sequence, [-1, state_size])\n",
    "logits = RNN_logits(states_reshaped, num_classes)\n",
    "logits = tf.reshape(logits, [batch_size, num_steps, -1])\n",
    "\n",
    "predictions = tf.nn.softmax(logits)\n",
    "\n",
    "# calculate a difference between predited and correct labels\n",
    "losses = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=labels_tensor)\n",
    "loss = tf.reduce_sum(losses) / batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch_size = ((len(train_data) // batch_size) - 1) // num_steps\n",
    "\n",
    "with tf.Session() as session:\n",
    "    \n",
    "  print(\"RNN type:\", rnn_type)\n",
    "  print()\n",
    "    \n",
    "  saver = tf.train.Saver()\n",
    "  saver.restore(session, \"language-rnn-0\")\n",
    "\n",
    "  session.run(tf.global_variables_initializer())\n",
    "    \n",
    "  #\n",
    "  input_coord = tf.train.Coordinator() \n",
    "  input_threads = tf.train.start_queue_runners(session, coord=input_coord)\n",
    "  #\n",
    "    \n",
    "  for epoch in range(1):\n",
    "        \n",
    "      total_loss = 0\n",
    "      total_time_steps = 0\n",
    "   \n",
    "      epoch_hidden_states = []\n",
    "      for state_pl in hidden_states:\n",
    "            epoch_hidden_states.append(np.zeros((2, batch_size, state_size)))\n",
    "\n",
    "      for step in range(epoch_size):\n",
    "\n",
    "        # build feed dict\n",
    "        feed_dict = {}\n",
    "        \n",
    "        for state_pl, state_val in zip(hidden_states, epoch_hidden_states):\n",
    "            feed_dict[state_pl] = state_val\n",
    "            \n",
    "        loss_val, epoch_hidden_states = session.run([loss, final_states], feed_dict=feed_dict)\n",
    "\n",
    "        total_loss += loss_val\n",
    "        total_time_steps += num_steps\n",
    "            \n",
    "      epoch_perplexity = np.exp(total_loss / total_time_steps)\n",
    "    \n",
    "      print(\"epoch {} - perplexity: {:.3f}\".format(epoch + 1, epoch_perplexity))\n",
    "    \n",
    "  #saver.save(session, \"language-rnn\", global_step=0)\n",
    "    \n",
    "  #  \n",
    "  input_coord.request_stop()\n",
    "  input_coord.join(input_threads)  \n",
    "  #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1\n",
    "\n",
    "rnn_type = \"LSTM\"\n",
    "tf.reset_default_graph()\n",
    "\n",
    "words_pl = tf.placeholder(tf.int32, shape=[batch_size, None])\n",
    "num_steps = tf.shape(words_pl)[1]\n",
    "\n",
    "embeddings = tf.get_variable(\"embeddings\", [num_classes, state_size])\n",
    "rnn_inputs = tf.nn.embedding_lookup(embeddings, words_pl)\n",
    "\n",
    "def build_layer(rnn_inputs, layer_idx):\n",
    "    \n",
    "    with tf.variable_scope(\"layer{}\".format(layer_idx)):\n",
    "    \n",
    "        # an initial hidden state zero-filled tensor\n",
    "        init_hidden_state = tf.random_uniform(minval=-1, maxval=1, shape=[2, batch_size, state_size])\n",
    "\n",
    "        # TODO: proc se to transponuje tam a zase zpatky?\n",
    "        states = tf.scan(LSTM_step, tf.transpose(rnn_inputs, [1,0,2]), initializer=init_hidden_state) \n",
    "        states = tf.transpose(states, [1,2,0,3])[0]\n",
    "\n",
    "        return states\n",
    "    \n",
    "sequence = rnn_inputs\n",
    "for layer_idx in range(num_layers):\n",
    "    sequence = build_layer(sequence, layer_idx)\n",
    "    \n",
    "states_reshaped = tf.reshape(sequence, [-1, state_size])\n",
    "logits = RNN_logits(states_reshaped, num_classes)\n",
    "logits = tf.reshape(logits, [batch_size, num_steps, -1])\n",
    "\n",
    "predictions = tf.argmax(logits, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_by_index(index):\n",
    "    for word, idx in vocab.items():\n",
    "        if idx == index:\n",
    "            return word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_by_index(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as session:\n",
    "    \n",
    "  saver = tf.train.Saver()\n",
    "  saver.restore(session, \"language-rnn-0\")\n",
    "    \n",
    "  inputs = [[0]]\n",
    "  for i in range(100):\n",
    "      val = session.run(predictions, feed_dict={words_pl: inputs})\n",
    "      inputs[0].append(val[0][-1])\n",
    "        \n",
    "  sentence = []\n",
    "  for word_idx in inputs[0]:\n",
    "    sentence.append(word_by_index(word_idx))\n",
    "    \n",
    "  for word in sentence:\n",
    "    print(word, end=\" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Further reading**\n",
    "\n",
    "  * Well explained LSTM: https://colah.github.io/posts/2015-08-Understanding-LSTMs/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
