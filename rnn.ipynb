{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MI-MVI tutorial 3 #\n",
    "\n",
    "In this tutorial, you will use **Recurrent Neural Networks (RNN)** to predict words in english text. Large neural language models have been immensely successful in language modelling tasks recently, so it's relevant for you to learn the basics.\n",
    "\n",
    " - Based on TF RNN tutorial: https://www.tensorflow.org/tutorials/recurrent\n",
    " - Pretty images an some thory come from: https://colah.github.io/posts/2015-08-Understanding-LSTMs/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A recurrent neural network can be thought of as multiple copies of the same network, each passing a message to a successor. Consider what happens if we unroll the loop:\n",
    "\n",
    "![RNN unroled](images/RNN-unrolled.png \"Structure of unrolled RNN.\")\n",
    "\n",
    "In the above diagram, a chunk of neural network, *A*, looks at some input *x_t* and outputs a value *h_t*. A loop allows information to be passed from one step of the network to the next. The *A* labelled boxes are elementary modules which vary with a particular type of RNN. In the standard (vanilla) RNN they look like:\n",
    "\n",
    "![RNN cenll](images/SimpleRNNcell.png)\n",
    "in comparison to the LSTM cell:\n",
    "![LSTM cell](images/LSTMcell.png)\n",
    "\n",
    "We will focus on the standard RNN cell. The yellow box with *tanh* is a neural network layer. It combines an input *x_t* and a previous state of the preceeding cell. We are going to build such cell from scratch, but you can use the **tf.contrib.rnn.RNNCell** abstract class for easier and faster implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Import** all packages that will be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os, sys, tarfile\n",
    "import collections\n",
    "from six.moves.urllib.request import urlretrieve\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download the [Penn Tree Bank (PTB)](https://catalog.ldc.upenn.edu/ldc99t42) dataset. We use an identical approach as in the last tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "url = 'http://www.fit.vutbr.cz/~imikolov/rnnlm/'\n",
    "data_root = 'data/rnn'\n",
    "last_percent_reported = None\n",
    "\n",
    "# make sure the dataset directory exists\n",
    "if not os.path.isdir(data_root):\n",
    "  os.makedirs(data_root)\n",
    "\n",
    "def download_progress_hook(count, blockSize, totalSize):\n",
    "  \"\"\"A hook to report the progress of a download. This is mostly intended for users with\n",
    "  slow internet connections. Reports every 5% change in download progress.\n",
    "  \"\"\"\n",
    "  global last_percent_reported\n",
    "  percent = int(count * blockSize * 100 / totalSize)\n",
    "\n",
    "  if last_percent_reported != percent:\n",
    "    if percent % 5 == 0:\n",
    "      sys.stdout.write(\"%s%%\" % percent)\n",
    "      sys.stdout.flush()\n",
    "    else:\n",
    "      sys.stdout.write(\".\")\n",
    "      sys.stdout.flush()\n",
    "      \n",
    "    last_percent_reported = percent\n",
    "    \n",
    "def maybe_download(filename, expected_bytes, force=False):\n",
    "  \"\"\"Download a file if not present, and make sure it's the right size.\"\"\"\n",
    "  dest_filename = os.path.join(data_root, filename)\n",
    "  if force or not os.path.exists(dest_filename):\n",
    "    print('Attempting to download:', filename) \n",
    "    filename, _ = urlretrieve(url + filename, dest_filename, reporthook=download_progress_hook)\n",
    "    print('\\nDownload Complete!')\n",
    "  statinfo = os.stat(dest_filename)\n",
    "  if statinfo.st_size == expected_bytes:\n",
    "    print('Found and verified', dest_filename)\n",
    "  else:\n",
    "    raise Exception(\n",
    "      'Failed to verify ' + dest_filename + '. Can you get to it with a browser?')\n",
    "  return dest_filename\n",
    "\n",
    "train_filename = maybe_download('simple-examples.tgz', 34869662)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to unpack the downloaded data. The desired content is in **data** subdirectory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def maybe_extract(filename, force=False):\n",
    "  root = os.path.splitext(os.path.splitext(filename)[0])[0]  # remove .tar.gz\n",
    "  if os.path.isdir(root) and not force:\n",
    "    # You may override by setting force=True.\n",
    "    print('%s already present - Skipping extraction of %s.' % (root, filename))\n",
    "  else:\n",
    "    print('Extracting data for %s. This may take a while. Please wait.' % root)\n",
    "    tar = tarfile.open(filename)\n",
    "    sys.stdout.flush()\n",
    "    tar.extractall(data_root)\n",
    "    tar.close()\n",
    "  data_folders = [\n",
    "    os.path.join(root, d) for d in sorted(os.listdir(root))\n",
    "    if os.path.isdir(os.path.join(root, d))]\n",
    "  \n",
    "  print(data_folders)\n",
    "  return data_folders\n",
    "  \n",
    "train_folders = maybe_extract(train_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we define a few helpers to manipulate the data. Because Tensorflow needs tensors of numbers we will represent words by indexes. For this requirement we build a vocabulary from a given file. The vocabulary will contain a key-value pairs of following meaning 'word':ID.\n",
    "\n",
    "A **ptb_raw_data** method loads all necessary files, creates vocabularies and transform content of train, validation and test datafiles to number sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _read_words(filename):\n",
    "  with tf.gfile.GFile(filename, \"r\") as f:\n",
    "    return f.read().replace(\"\\n\", \"<eos>\").split()\n",
    "\n",
    "def _build_vocab(filename, wordsLimit=None):\n",
    "  data = _read_words(filename)\n",
    "  counter = collections.Counter(data)\n",
    "  count_pairs = sorted(counter.items(), key=lambda x: (-x[1], x[0]))\n",
    "  if (wordsLimit!=None):\n",
    "        count_pairs = count_pairs[0:wordsLimit]\n",
    "  words, _ = list(zip(*count_pairs))\n",
    "  word_to_id = dict(zip(words, range(len(words))))\n",
    "  return word_to_id\n",
    "\n",
    "def _file_to_word_ids(filename, word_to_id):\n",
    "  data = _read_words(filename)\n",
    "  return [word_to_id[word] for word in data if word in word_to_id]\n",
    "\n",
    "def ptb_raw_data(data_path=None, wordsLimit=None):\n",
    "  \"\"\"Load PTB raw data from data directory \"data_path\".\n",
    "  Reads PTB text files, converts strings to integer ids, and performs mini-batching of the inputs.\n",
    "  Args:\n",
    "    data_path: string path to the directory where simple-examples.tgz has been extracted.\n",
    "  Returns:\n",
    "    tuple (train_data, valid_data, test_data, vocabulary)\n",
    "    where each of the data objects can be passed to PTBIterator.\n",
    "  \"\"\"\n",
    "\n",
    "  train_path = os.path.join(data_path, \"ptb.train.txt\")\n",
    "  valid_path = os.path.join(data_path, \"ptb.valid.txt\")\n",
    "  test_path = os.path.join(data_path, \"ptb.test.txt\")\n",
    "\n",
    "  word_to_id = _build_vocab(train_path, wordsLimit)\n",
    "  train_data = _file_to_word_ids(train_path, word_to_id)\n",
    "  valid_data = _file_to_word_ids(valid_path, word_to_id)\n",
    "  test_data = _file_to_word_ids(test_path, word_to_id)\n",
    "  vocabulary = len(word_to_id)\n",
    "  return train_data, valid_data, test_data, vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us inspect the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# in case of very slow learning we can trim an input dictionary (10000 is a base)\n",
    "wordsLimit=10000\n",
    "\n",
    "train_data, valid_data, test_data, vocabulary = ptb_raw_data(os.path.join(data_root, 'simple-examples/data'), wordsLimit)\n",
    "vocab = _build_vocab(os.path.join(data_root, 'simple-examples','data','ptb.test.txt'), wordsLimit)\n",
    "firstitems = {k: vocab[k] for k in sorted(vocab.keys())[:30]}\n",
    "\n",
    "print('train data len:', len(train_data))\n",
    "print('validation data len:', len(valid_data))\n",
    "print('test data len:', len(test_data))\n",
    "print('vocabulary item count:', vocabulary)\n",
    "print('the first 30 vocabulary items:', firstitems)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ptb_producer(raw_data, batch_size, num_steps, name=None):\n",
    "  \"\"\"\n",
    "  Iterate on the raw PTB data.\n",
    "  This chunks up raw_data into batches of examples and returns Tensors that are drawn from these batches.\n",
    "  Args:\n",
    "    raw_data: one of the raw data outputs from ptb_raw_data.\n",
    "    batch_size: int, the batch size.\n",
    "    num_steps: int, the number of unrolls.\n",
    "    name: the name of this operation (optional).\n",
    "  Returns:\n",
    "    A pair of Tensors, each shaped [batch_size, num_steps]. The second element\n",
    "    of the tuple is the same data time-shifted to the right by one.\n",
    "  Raises:\n",
    "    tf.errors.InvalidArgumentError: if batch_size or num_steps are too high.\n",
    "  \"\"\"\n",
    "\n",
    "  with tf.name_scope(name, \"PTBProducer\", [raw_data, batch_size, num_steps]):\n",
    "    raw_data = tf.convert_to_tensor(raw_data, name=\"raw_data\", dtype=tf.int32)\n",
    "\n",
    "    data_len = tf.size(raw_data)\n",
    "    batch_len = data_len // batch_size\n",
    "    data = tf.reshape(raw_data[0 : batch_size * batch_len], [batch_size, batch_len])\n",
    "\n",
    "    epoch_size = (batch_len - 1) // num_steps\n",
    "    assertion = tf.assert_positive(epoch_size, message=\"epoch_size == 0, decrease batch_size or num_steps\")\n",
    "    with tf.control_dependencies([assertion]):\n",
    "      epoch_size = tf.identity(epoch_size, name=\"epoch_size\")\n",
    "\n",
    "    i = tf.train.range_input_producer(epoch_size, shuffle=False).dequeue()\n",
    "    x = tf.strided_slice(data, [0, i * num_steps], [batch_size, (i + 1) * num_steps])\n",
    "    x.set_shape([batch_size, num_steps])\n",
    "    y = tf.strided_slice(data, [0, i * num_steps + 1],[batch_size, (i + 1) * num_steps + 1])\n",
    "    y.set_shape([batch_size, num_steps])\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is a time to put all together... "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vanilla RNN and LSTM ##\n",
    "\n",
    "Preparing the building blocks for both types of RNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# it says how many floats will represent coordinates of given word in embeddings\n",
    "# it is a \"width\" of embeddings\n",
    "state_size = 200\n",
    "\n",
    "# default value for weights in RNN_logits\n",
    "init_scale = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def RNN_logits(states, output_size):\n",
    "  \"\"\"\n",
    "  Create a final classification layer that is ran on top of an RNN.\n",
    "\n",
    "  :param states:             Output states of an RNN.\n",
    "  :param outputs_size:       Number of classes to predict.\n",
    "  :return:                   Logits.\n",
    "  \"\"\"\n",
    "        \n",
    "  # RNN parameters\n",
    "  V = tf.get_variable('V', shape=[state_size, output_size], \n",
    "                        initializer=tf.random_uniform_initializer(minval=-init_scale, maxval=init_scale))\n",
    "  bo = tf.get_variable('bl', shape=[output_size], initializer=tf.constant_initializer(0.))\n",
    "    \n",
    "  # calculate logits\n",
    "  return tf.matmul(states, V) + bo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![RNN cenll](images/SimpleRNNcell.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# one piece of unrolled vanilla RNN\n",
    "def RNN_step(previous_hidden_state, input_tensor):\n",
    "  \"\"\"\n",
    "  Unroll an RNN for a single step.\n",
    "  \n",
    "  :param previous_hidden_state:         Hidden state of the previous time step of the RNN.\n",
    "  :param input_tensor:                  Input for the current time step.\n",
    "  :return                               New hidden state.\n",
    "  \"\"\"\n",
    "    \n",
    "  # RNN parameters\n",
    "  W = tf.get_variable(\"W\", shape=[state_size, state_size], \n",
    "                        initializer=tf.random_uniform_initializer(minval=-init_scale, maxval=init_scale))\n",
    "  U = tf.get_variable(\"U\", shape=[state_size, state_size], \n",
    "                        initializer=tf.random_uniform_initializer(minval=-init_scale, maxval=init_scale))\n",
    "  b = tf.get_variable(\"b\", shape=[state_size], initializer=tf.constant_initializer(0.))\n",
    "    \n",
    "  # calculate new hidden state\n",
    "  hidden_state = tf.tanh(tf.matmul(previous_hidden_state, W) + tf.matmul(input_tensor,U) + b)\n",
    "    \n",
    "  return hidden_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![LSTM cell](images/LSTMcell.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# one piece of unrolled vanilla RNN\n",
    "def LSTM_step(previous_hidden_state, input_tensor):\n",
    "  \"\"\"\n",
    "  Unroll an LSTM for a single step.\n",
    "  \n",
    "  :param previous_hidden_state:         Hidden state of the previous time step of the LSTM.\n",
    "  :param input_tensor:                  Input for the current time step.\n",
    "  :return                               New hidden state.\n",
    "  \"\"\"\n",
    "    \n",
    "  # weights for input\n",
    "  W = tf.get_variable('W', shape=[4, state_size, state_size], \n",
    "                        initializer=tf.random_uniform_initializer(minval=-init_scale, maxval=init_scale))\n",
    "  # weights for previous hidden state\n",
    "  U = tf.get_variable('U', shape=[4, state_size, state_size], \n",
    "                        initializer=tf.random_uniform_initializer(minval=-init_scale, maxval=init_scale))\n",
    "    \n",
    "  bi = tf.get_variable(\"bi\", shape=[state_size], initializer=tf.constant_initializer(0.))\n",
    "  bf = tf.get_variable(\"bf\", shape=[state_size], initializer=tf.constant_initializer(0.))\n",
    "  bo = tf.get_variable(\"bo\", shape=[state_size], initializer=tf.constant_initializer(0.))\n",
    "  bc = tf.get_variable(\"bc\", shape=[state_size], initializer=tf.constant_initializer(0.))\n",
    "    \n",
    "  # gather previous internal state and output state\n",
    "  state, cell = tf.unstack(previous_hidden_state)\n",
    "    \n",
    "  # gates\n",
    "  input_gate = tf.sigmoid(tf.matmul(input_tensor, U[0]) + tf.matmul(state, W[0]) + bi)\n",
    "  forget_gate = tf.sigmoid(tf.matmul(input_tensor, U[1]) + tf.matmul(state, W[1]) + bf)\n",
    "  output_gate = tf.sigmoid(tf.matmul(input_tensor, U[2]) + tf.matmul(state, W[2]) + bo)\n",
    "  gate_weights = tf.tanh(tf.matmul(input_tensor, U[3]) + tf.matmul(state, W[3]) + bc)\n",
    "    \n",
    "  # new internal cell state\n",
    "  cell = cell * forget_gate + gate_weights * input_gate\n",
    "    \n",
    "  # output state\n",
    "  state = tf.tanh(cell) * output_gate\n",
    "  return tf.stack([state, cell])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because we will dive into the implementation of vanilla and LSTM RNN it will come handy to understand **tf.transpose** by playing with it for a while. A **tf.transpose** permutates dimensions of a tensor. You will need to specify an order (permutation) of all dimensions (counted from 0 to N-1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "c = tf.constant([[[ 1,  2,  3],\n",
    "                  [ 4,  5,  6]],\n",
    "                 [[ 7,  8,  9],\n",
    "                  [10, 11, 12]]])\n",
    "\n",
    "ctr = tf.transpose(c, perm=[1, 0, 2])\n",
    "\n",
    "with tf.Session() as session:\n",
    "  res = session.run(ctr)\n",
    "  print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And for the better understanding of the **embedings** and **rnn_inputs** variables see:\n",
    "\n",
    "![Embeddings and rnn_internals](images/rnn-internals.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vanilla RNN ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_classes = vocabulary\n",
    "num_steps = 20\n",
    "batch_size = 20\n",
    "state_size = 200\n",
    "max_gradient_norm = 5\n",
    "learning_rate = 1.0\n",
    "\n",
    "# simple RNN cell\n",
    "rnn_type = \"vanilla\"\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# take a subset of data\n",
    "input_tensor, labels_tensor = ptb_producer(train_data, batch_size=batch_size, num_steps=num_steps)\n",
    "\n",
    "# with no specification of variable initializer the tf.glorot_uniform_initializer is used\n",
    "# it is also called Xavier uniform initializer.\n",
    "embeddings = tf.get_variable(\"embeddings\", [num_classes, state_size])\n",
    "# rnn input contains \"coordinates\" in embeddings of IDs from input tensor\"\n",
    "# it is a way how to effectively encode integers representing words to form\n",
    "# which is familiar for NN.\n",
    "rnn_inputs = tf.nn.embedding_lookup(embeddings, input_tensor)\n",
    "\n",
    "# in learning phase the hidden state is a zero-filled tensor\n",
    "# in case of retrieving we supply initial tensor\n",
    "init_hidden_state = tf.placeholder(shape=[batch_size, state_size], dtype=tf.float32)\n",
    "\n",
    "states = tf.scan(RNN_step, tf.transpose(rnn_inputs, [1,0,2]), initializer=init_hidden_state) \n",
    "states = tf.transpose(states, [1,0,2])\n",
    "\n",
    "# a tf.matmul operator in RNN_logits do not accept tensors, only matrixes\n",
    "# so we need to flatten our tensor into 2D array\n",
    "states_reshaped = tf.reshape(states, [-1, state_size])\n",
    "# process that as matrix\n",
    "logits = RNN_logits(states_reshaped, num_classes)\n",
    "# reconstruct tensor\n",
    "logits = tf.reshape(logits, [batch_size, num_steps, -1])\n",
    "\n",
    "predictions = tf.nn.softmax(logits)\n",
    "\n",
    "# calculate a difference between predicted and correct labels\n",
    "losses = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=labels_tensor)\n",
    "loss = tf.reduce_mean(losses)\n",
    "\n",
    "# For reasons of a gradient clipping method see: http://arxiv.org/pdf/1211.5063.pdf\n",
    "trainable_vars = tf.trainable_variables()\n",
    "grads, _ = tf.clip_by_global_norm(tf.gradients(loss, trainable_vars), max_gradient_norm)\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "train_op = optimizer.apply_gradients(zip(grads, trainable_vars), \n",
    "                                     global_step=tf.contrib.framework.get_or_create_global_step())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_classes = vocabulary\n",
    "num_steps = 20\n",
    "batch_size = 20\n",
    "state_size = 200\n",
    "max_gradient_norm = 5\n",
    "learning_rate = 1.0\n",
    "\n",
    "# LSTM cell\n",
    "rnn_type = \"LSTM\"\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# take a subset of data\n",
    "input_tensor, labels_tensor = ptb_producer(train_data, batch_size=batch_size, num_steps=num_steps)\n",
    "\n",
    "# with no specification of variable initializer the tf.glorot_uniform_initializer is used\n",
    "# it is also called Xavier uniform initializer.\n",
    "embeddings = tf.get_variable(\"embeddings\", [num_classes, state_size])\n",
    "# rnn input contains \"coordinates\" in embeddings of IDs from input tensor\"\n",
    "# it is a way how to effectively encode integers representing words to form\n",
    "# which is familiar for NN.\n",
    "rnn_inputs = tf.nn.embedding_lookup(embeddings, input_tensor)\n",
    "\n",
    "# an initial hidden state zero-filled tensor\n",
    "init_hidden_state = tf.placeholder(shape=[2, batch_size, state_size], dtype=tf.float32, name='initial_state')\n",
    "\n",
    "states = tf.scan(LSTM_step, tf.transpose(rnn_inputs, [1,0,2]), initializer=init_hidden_state) \n",
    "states = tf.transpose(states, [1,2,0,3])[0]\n",
    "\n",
    "# a tf.matmul operator in RNN_logits do not accept tensors, only matrixes\n",
    "# so we need to flatten our tensor into 2D array\n",
    "states_reshaped = tf.reshape(states, [-1, state_size])\n",
    "# process that as matrix\n",
    "logits = RNN_logits(states_reshaped, num_classes)\n",
    "# reconstruct tensor\n",
    "logits = tf.reshape(logits, [batch_size, num_steps, -1])\n",
    "\n",
    "predictions = tf.nn.softmax(logits)\n",
    "\n",
    "# calculate a difference between predicted and correct labels\n",
    "losses = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=labels_tensor)\n",
    "loss = tf.reduce_mean(losses)\n",
    "\n",
    "# For reasons of a gradient clipping method see: http://arxiv.org/pdf/1211.5063.pdf\n",
    "trainable_vars = tf.trainable_variables()\n",
    "grads, _ = tf.clip_by_global_norm(tf.gradients(loss, trainable_vars), max_gradient_norm)\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "train_op = optimizer.apply_gradients(zip(grads, trainable_vars), \n",
    "                                     global_step=tf.contrib.framework.get_or_create_global_step())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can choose to train either the **vanilla** or the **LSTM** version of a Recurrent Neural Network by running one of the graph definitions above. You can notice that vanilla LSTM is much harder to train even in this small-scale experiment. The difference between vanilla and LSTM becomes much more pronounced when you experiment with larger RNN. Moreover, there are many tasks that are impossible to solve by vanilla RNNs (see [Sepp Hochreiter's and Jürgen Schmidhuber's seminal paper for examples](http://www.bioinf.jku.at/publications/older/2604.pdf))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_training_steps = 101\n",
    "\n",
    "# depending on type we prepare an appropriate initial hidden state tensor\n",
    "def create_feed_dict(rnn_type):\n",
    "  if rnn_type == \"vanilla\":\n",
    "    return np.zeros([batch_size, state_size])\n",
    "  else:\n",
    "    return np.zeros([2, batch_size, state_size])\n",
    "    \n",
    "with tf.Session() as session:\n",
    "  print(\"RNN type: \", rnn_type)\n",
    "  session.run(tf.global_variables_initializer())\n",
    "    \n",
    "  # black magic with a threading ;-)\n",
    "  input_coord = tf.train.Coordinator() \n",
    "  input_threads = tf.train.start_queue_runners(session, coord=input_coord)\n",
    "    \n",
    "  for step in range(num_training_steps):\n",
    "    \n",
    "    loss_val, _ = session.run([loss, train_op], feed_dict={\n",
    "      init_hidden_state: create_feed_dict(rnn_type)\n",
    "    })\n",
    "    \n",
    "    input_vals, labels_vals = session.run([input_tensor, labels_tensor])\n",
    "\n",
    "    if step > 0 and step % 10 == 0:\n",
    "        print(\"step:\", step)\n",
    "        print(\"loss:\", loss_val)\n",
    "        print()\n",
    "    \n",
    "  # wait for the finalization of a multithreaded run\n",
    "  input_coord.request_stop()\n",
    "  input_coord.join(input_threads)  \n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task (bonus points) ###\n",
    "\n",
    "Visualize how the loss changes during the training of your Recurrent Neural Network using [Tensorboard](https://www.tensorflow.org/get_started/summaries_and_tensorboard). Use MI-MVI tutorial 2 as a reference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Optional) Large multi-layer LSTM ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far, you have experimented with small models and trained them for tens or hunders of iterations. However, models that are used in practice usually contain **millions** of weights and are trained for **hundreds of thousands of steps**.\n",
    "\n",
    "You can see an implementation of such a model bellow. We have made several changes to increase its performace:\n",
    "\n",
    "* truncated back-propagation\n",
    "* stack multiple layers of LSTM cells on top of each other\n",
    "* employ smart learning rate schedule\n",
    "\n",
    "These techniques are beyond the scope of this course but Google Scholar is your friend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# configuration\n",
    "num_classes = vocabulary\n",
    "max_gradient_norm = 5\n",
    "hidden_size = 200\n",
    "num_steps = 20\n",
    "batch_size = 20\n",
    "num_layers = 2\n",
    "\n",
    "learning_rate = 1.0\n",
    "learning_rate_decay = 0.5\n",
    "epoch_end_decay = 4\n",
    "\n",
    "num_epochs = 13\n",
    "\n",
    "rnn_type = \"LSTM\"\n",
    "tf.reset_default_graph()\n",
    "\n",
    "input_tensor, labels_tensor = ptb_producer(train_data, batch_size=batch_size, num_steps=num_steps)\n",
    "\n",
    "embeddings = tf.get_variable(\"embeddings\", [num_classes, state_size])\n",
    "rnn_inputs = tf.nn.embedding_lookup(embeddings, input_tensor)\n",
    "\n",
    "def build_layer(rnn_inputs, layer_idx):\n",
    "    \n",
    "  with tf.variable_scope(\"layer{}\".format(layer_idx)):\n",
    "    \n",
    "    # truncated backprop\n",
    "    hidden_state = tf.placeholder(tf.float32, shape=[2, batch_size, state_size])\n",
    "        \n",
    "    states = tf.scan(LSTM_step, tf.transpose(rnn_inputs, [1,0,2]), initializer=hidden_state) \n",
    "    states = tf.transpose(states, [1,2,0,3])\n",
    "       \n",
    "    return states, hidden_state\n",
    "    \n",
    "sequence = rnn_inputs\n",
    "\n",
    "final_states = []\n",
    "hidden_states = []\n",
    "\n",
    "# multi-layer LSTM\n",
    "for layer_idx in range(num_layers):\n",
    "  states, hidden_state = build_layer(sequence, layer_idx)\n",
    "  final_states.append(states[:, :, -1, :])\n",
    "  hidden_states.append(hidden_state)\n",
    "    \n",
    "  sequence = states[0]\n",
    "    \n",
    "states_reshaped = tf.reshape(sequence, [-1, state_size])\n",
    "logits = RNN_logits(states_reshaped, num_classes)\n",
    "logits = tf.reshape(logits, [batch_size, num_steps, -1])\n",
    "\n",
    "predictions = tf.nn.softmax(logits)\n",
    "\n",
    "losses = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=labels_tensor)\n",
    "loss = tf.reduce_sum(losses) / batch_size\n",
    "\n",
    "# learning rate schedule\n",
    "learning_rate_tensor = tf.Variable(learning_rate, name=\"learning_rate\")\n",
    "learning_rate_pl = tf.placeholder(tf.float32, name=\"learning_rate_pl\")\n",
    "assign_learning_rate = tf.assign(learning_rate_tensor, learning_rate_pl)\n",
    "\n",
    "trainable_vars = tf.trainable_variables()\n",
    "grads, _ = tf.clip_by_global_norm(tf.gradients(loss, trainable_vars), max_gradient_norm)\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate_tensor)\n",
    "train_op = optimizer.apply_gradients(zip(grads, trainable_vars), \n",
    "                                     global_step=tf.contrib.framework.get_or_create_global_step())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "epoch_size = ((len(train_data) // batch_size) - 1) // num_steps\n",
    "\n",
    "with tf.Session() as session:\n",
    "    \n",
    "  print(\"RNN type: \", rnn_type)\n",
    "  print()\n",
    "    \n",
    "  saver = tf.train.Saver()\n",
    "\n",
    "  session.run(tf.global_variables_initializer())\n",
    "    \n",
    "  input_coord = tf.train.Coordinator() \n",
    "  input_threads = tf.train.start_queue_runners(session, coord=input_coord)\n",
    "    \n",
    "  # train for several epochs (1 epochs = 1 pass throught the entire dataset)\n",
    "  for epoch in range(num_epochs):\n",
    "      \n",
    "    # decay (lower) learning rate after each epoch\n",
    "    learning_rate_decay = learning_rate_decay ** max(epoch + 1 - epoch_end_decay, 0.0)\n",
    "    session.run(assign_learning_rate, feed_dict={\n",
    "      learning_rate_pl: learning_rate * learning_rate_decay\n",
    "    })\n",
    "        \n",
    "    total_loss = 0\n",
    "    total_time_steps = 0\n",
    "   \n",
    "    epoch_hidden_states = []\n",
    "    for state_pl in hidden_states:\n",
    "       epoch_hidden_states.append(np.zeros((2, batch_size, state_size)))\n",
    "\n",
    "    for step in range(epoch_size):\n",
    "\n",
    "      # build feed dict\n",
    "      feed_dict = {}\n",
    "        \n",
    "      # remember states from the previous training step\n",
    "      for state_pl, state_val in zip(hidden_states, epoch_hidden_states):\n",
    "        feed_dict[state_pl] = state_val\n",
    "            \n",
    "      loss_val, _, epoch_hidden_states = session.run([loss, train_op, final_states], feed_dict=feed_dict)\n",
    "\n",
    "      # store these to calculate per-epoch perplexity\n",
    "      total_loss += loss_val\n",
    "      total_time_steps += num_steps\n",
    "            \n",
    "      epoch_perplexity = np.exp(total_loss / total_time_steps)\n",
    "    \n",
    "    print(\"epoch {} - perplexity: {:.3f}\".format(epoch + 1, epoch_perplexity))\n",
    "    \n",
    "  # save the final model\n",
    "  saver.save(session, \"language-rnn\", global_step=0)\n",
    "     \n",
    "  input_coord.request_stop()\n",
    "  input_coord.join(input_threads)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Language Modelling ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we would like to show you what a medium-size neural language model can do. You can load two models, small and big, and let them finish sentences for you. We thank Showmax for letting us train the networks on their hardware."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Small Model ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The small model consists of two layers of 200 LSTM cells trained for about 10 minutes on a high-end GPU. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 1\n",
    "\n",
    "model_type = \"small\"\n",
    "num_layers = 2\n",
    "state_size = 200\n",
    "\n",
    "rnn_type = \"LSTM\"\n",
    "tf.reset_default_graph()\n",
    "\n",
    "words_pl = tf.placeholder(tf.int32, shape=[batch_size, None])\n",
    "num_steps = tf.shape(words_pl)[1]\n",
    "\n",
    "embeddings = tf.get_variable(\"embeddings\", [num_classes, state_size])\n",
    "rnn_inputs = tf.nn.embedding_lookup(embeddings, words_pl)\n",
    "\n",
    "def build_layer(rnn_inputs, layer_idx):\n",
    "    \n",
    "  with tf.variable_scope(\"layer{}\".format(layer_idx)):\n",
    "    \n",
    "    hidden_state = tf.placeholder(tf.float32, shape=[2, batch_size, state_size])\n",
    "        \n",
    "    states = tf.scan(LSTM_step, tf.transpose(rnn_inputs, [1,0,2]), initializer=hidden_state) \n",
    "    states = tf.transpose(states, [1,2,0,3])\n",
    "       \n",
    "    return states, hidden_state\n",
    "    \n",
    "sequence = rnn_inputs\n",
    "\n",
    "final_states = []\n",
    "hidden_states = []\n",
    "\n",
    "for layer_idx in range(num_layers):\n",
    "  states, hidden_state = build_layer(sequence, layer_idx)\n",
    "  final_states.append(states[:, :, -1, :])\n",
    "  hidden_states.append(hidden_state)\n",
    "    \n",
    "  sequence = states[0]\n",
    "    \n",
    "states_reshaped = tf.reshape(sequence, [-1, state_size])\n",
    "logits = RNN_logits(states_reshaped, num_classes)\n",
    "logits = tf.reshape(logits, [batch_size, num_steps, -1])\n",
    "\n",
    "predictions = tf.argmax(logits, -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Large model ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 1\n",
    "\n",
    "model_type = \"large\"\n",
    "num_layers = 2\n",
    "keep_prob = 0.5\n",
    "state_size = 650\n",
    "\n",
    "rnn_type = \"LSTM\"\n",
    "tf.reset_default_graph()\n",
    "\n",
    "words_pl = tf.placeholder(tf.int32, shape=[batch_size, None])\n",
    "num_steps = tf.shape(words_pl)[1]\n",
    "\n",
    "embeddings = tf.get_variable(\"embeddings\", [num_classes, state_size])\n",
    "rnn_inputs = tf.nn.embedding_lookup(embeddings, words_pl)\n",
    "\n",
    "def build_layer(rnn_inputs, layer_idx):\n",
    "    \n",
    "  with tf.variable_scope(\"layer{}\".format(layer_idx)):\n",
    "    \n",
    "    hidden_state = tf.placeholder(tf.float32, shape=[2, batch_size, state_size])\n",
    "        \n",
    "    states = tf.scan(LSTM_step, tf.transpose(rnn_inputs, [1,0,2]), initializer=hidden_state) \n",
    "    states = tf.transpose(states, [1,2,0,3])\n",
    "       \n",
    "    return states, hidden_state\n",
    "    \n",
    "sequence = rnn_inputs\n",
    "is_training = tf.placeholder(tf.bool)\n",
    "\n",
    "final_states = []\n",
    "hidden_states = []\n",
    "\n",
    "for layer_idx in range(num_layers):\n",
    "    \n",
    "  sequence = tf.layers.dropout(sequence, rate=keep_prob, training=is_training)\n",
    "    \n",
    "  states, hidden_state = build_layer(sequence, layer_idx)\n",
    "  final_states.append(states[:, :, -1, :])\n",
    "  hidden_states.append(hidden_state)\n",
    "    \n",
    "  sequence = states[0]\n",
    "    \n",
    "sequence = tf.layers.dropout(sequence, rate=keep_prob, training=is_training)\n",
    "    \n",
    "states_reshaped = tf.reshape(sequence, [-1, state_size])\n",
    "logits = RNN_logits(states_reshaped, num_classes)\n",
    "logits = tf.reshape(logits, [batch_size, num_steps, -1])\n",
    "\n",
    "predictions = tf.argmax(logits, -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The large model is made of two layers of 650 LSTM cells regularized using dropout. The training time is about 2 hours on a single high-end GPU."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Sentences ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def word_by_index(index):\n",
    "  \"\"\"\n",
    "  Find a word in the vocabulary by its index.\n",
    "  \n",
    "  :param index:    Index of the word to find.\n",
    "  :return:         A string.\n",
    "  \"\"\"\n",
    "    \n",
    "  for word, idx in vocab.items():\n",
    "    if idx == index:\n",
    "      return word\n",
    "        \n",
    "def parse_sentence(sentence):\n",
    "  \"\"\"\n",
    "  Transform a sentence into word indexes.\n",
    "  \n",
    "  :param sentence:  A sentence.\n",
    "  :return:          An array of word indexes.\n",
    "  \"\"\"\n",
    "  \n",
    "  words = sentence.split(\" \")\n",
    "  indexes = []\n",
    "    \n",
    "  for word in words:\n",
    "    \n",
    "    if word in vocab:\n",
    "      indexes.append(vocab[word])\n",
    "    else:\n",
    "      indexes.append(vocab[\"<unk>\"])\n",
    "\n",
    "  return indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# PUT THE BEGGINING OF THE SENTENCE YOU WANT TO COMPLETE HERE\n",
    "input_sentence = \"the meaning of life is\"\n",
    "\n",
    "words_to_generate = 50\n",
    "# the network sometimes outputs an \"<eos>\" token that marks the end of a sentence\n",
    "# you can choose to ignore it for more interesting outputs\n",
    "end_early = False             \n",
    "\n",
    "with tf.Session() as session:\n",
    "    \n",
    "  saver = tf.train.Saver()\n",
    "\n",
    "  # load one of the pretrained models\n",
    "  if model_type == \"small\":\n",
    "    model_path = \"models/language-rnn-small\"\n",
    "  else:\n",
    "    model_path = \"models/language-rnn-medium\"\n",
    "    \n",
    "  saver.restore(session, model_path)\n",
    "    \n",
    "  # start with a random hidden state\n",
    "  sentence_hidden_states = []\n",
    "  for state_pl in hidden_states:\n",
    "    sentence_hidden_states.append(np.random.uniform(low=-1, high=1, size=(2, batch_size, state_size)))\n",
    "    \n",
    "  inputs = [parse_sentence(input_sentence)]\n",
    "  for i in range(words_to_generate):\n",
    "        \n",
    "    # give the network the sequence generated so far and its last hidden state\n",
    "    feed_dict = {\n",
    "      words_pl: inputs\n",
    "    }\n",
    "    \n",
    "    if model_type == \"large\":\n",
    "      feed_dict[is_training] = False\n",
    "        \n",
    "    for state_pl, state_val in zip(hidden_states, sentence_hidden_states):\n",
    "      feed_dict[state_pl] = state_val\n",
    "                    \n",
    "    # generate a new word\n",
    "    pred, sentence_hidden_states = \\\n",
    "      session.run([predictions, final_states], feed_dict=feed_dict)\n",
    "        \n",
    "    last_word = pred[0][-1]\n",
    "    \n",
    "    # maybe end the generation when <eos> is encountered\n",
    "    if end_early and last_word == vocab[\"<eos>\"]:\n",
    "      break\n",
    "        \n",
    "    # store the last generated word\n",
    "    inputs[0].append(last_word)\n",
    "        \n",
    "  sentence = []\n",
    "  for word_idx in inputs[0]:\n",
    "        \n",
    "    # remove control characters (<unk> = unknown, <eos> = end of sentence)\n",
    "    if word_idx not in [vocab[\"<unk>\"], vocab[\"N\"], vocab[\"<eos>\"]]:\n",
    "      sentence.append(word_by_index(word_idx))\n",
    "    \n",
    "  for word in sentence:\n",
    "    print(word, end=\" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are not convinced, there is a whole [book](https://www.kosmas.cz/knihy/216522/poezie-umeleho-sveta/) generated by a neural network. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Further reading**\n",
    "\n",
    "  * Well explained LSTM: https://colah.github.io/posts/2015-08-Understanding-LSTMs/\n",
    "  * http://blog.echen.me/2017/05/30/exploring-lstms/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
