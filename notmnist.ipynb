{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# MI-MVI tutorial 2 #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-size:larger;\">In the first tutorial, we introduced you to **Tensorflow**, a Deep Learning framework. You learned how to **define a computation graph**, and **create and initialize a Session**. Finally, you trained a simple classification model on a dataset of digits called **MNIST**.</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-size:larger;\">In this tutorial, you will download and preprocess a new dataset from scratch. Furthermore, we will show you how to use some advanced Tensorflow features like saving and loading models as well as visualizing the training. Lastly, you will experiment with various neural network architectures to obtain a good classifier.</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-size:larger;\">We thank the authors of this [Udacity tutorial](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/examples/udacity) which was the main inspiration for this tutorial. We have reused some of their code snippets.</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: the notMNIST dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![notMNIST example](images/notMNIST.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-size:larger;\">The [notMNIST](http://yaroslavvb.blogspot.cz/2011/09/notmnist-dataset.html) created by [Yaroslav Bulatov](https://www.blogger.com/profile/06139256691290554110) contains pictures of **letters from A - J** created from a multitude of publicly available fonts. The letters are in the same fromat as MNIST - 28x28 grayscale pictures. The dataset is harder than MNIST but small enough to be used on a laptop making it a perfect dataset for small-scale experiments.</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-size:larger;\">Let's start by downloading and preprocessing the dataset. By the end of this section, you should have a file containing 110000 preprocessed pictures of letters</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-size:larger;\">**Import** all packages that will be used.</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, tarfile\n",
    "from six.moves.urllib.request import urlretrieve\n",
    "import numpy as np\n",
    "from scipy import ndimage\n",
    "from six.moves import cPickle as pickle\n",
    "from IPython.display import display, Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-size:larger;\">**Download** the dataset.</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://commondatastorage.googleapis.com/books1000/'\n",
    "last_percent_reported = None\n",
    "data_root = 'data/notMNIST'\n",
    "\n",
    "# make sure the dataset directory exists\n",
    "if not os.path.isdir(data_root):\n",
    "  os.makedirs(data_root)\n",
    "\n",
    "def download_progress_hook(count, blockSize, totalSize):\n",
    "  \"\"\"A hook to report the progress of a download. This is mostly intended for users with\n",
    "  slow internet connections. Reports every 5% change in download progress.\n",
    "  \"\"\"\n",
    "  global last_percent_reported\n",
    "  percent = int(count * blockSize * 100 / totalSize)\n",
    "\n",
    "  if last_percent_reported != percent:\n",
    "    if percent % 5 == 0:\n",
    "      sys.stdout.write(\"%s%%\" % percent)\n",
    "      sys.stdout.flush()\n",
    "    else:\n",
    "      sys.stdout.write(\".\")\n",
    "      sys.stdout.flush()\n",
    "      \n",
    "    last_percent_reported = percent\n",
    "        \n",
    "def maybe_download(filename, expected_bytes, force=False):\n",
    "  \"\"\"Download a file if not present, and make sure it's the right size.\"\"\"\n",
    "  dest_filename = os.path.join(data_root, filename)\n",
    "  if force or not os.path.exists(dest_filename):\n",
    "    print('Attempting to download:', filename) \n",
    "    filename, _ = urlretrieve(url + filename, dest_filename, reporthook=download_progress_hook)\n",
    "    print('\\nDownload Complete!')\n",
    "  statinfo = os.stat(dest_filename)\n",
    "  if statinfo.st_size == expected_bytes:\n",
    "    print('Found and verified', dest_filename)\n",
    "  else:\n",
    "    raise Exception(\n",
    "      'Failed to verify ' + dest_filename + '. Can you get to it with a browser?')\n",
    "  return dest_filename\n",
    "\n",
    "train_filename = maybe_download('notMNIST_large.tar.gz', 247336696)\n",
    "test_filename = maybe_download('notMNIST_small.tar.gz', 8458043)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-size:larger;\">The dataset was downloaded as two tarballs. **Extract** both of them.</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 10\n",
    "np.random.seed(133)\n",
    "\n",
    "def maybe_extract(filename, force=False):\n",
    "  root = os.path.splitext(os.path.splitext(filename)[0])[0]  # remove .tar.gz\n",
    "  if os.path.isdir(root) and not force:\n",
    "    # You may override by setting force=True.\n",
    "    print('%s already present - Skipping extraction of %s.' % (root, filename))\n",
    "  else:\n",
    "    print('Extracting data for %s. This may take a while. Please wait.' % root)\n",
    "    tar = tarfile.open(filename)\n",
    "    sys.stdout.flush()\n",
    "    tar.extractall(data_root)\n",
    "    tar.close()\n",
    "  data_folders = [\n",
    "    os.path.join(root, d) for d in sorted(os.listdir(root))\n",
    "    if os.path.isdir(os.path.join(root, d))]\n",
    "  if len(data_folders) != num_classes:\n",
    "    raise Exception(\n",
    "      'Expected %d folders, one per class. Found %d instead.' % (\n",
    "        num_classes, len(data_folders)))\n",
    "  print(data_folders)\n",
    "  return data_folders\n",
    "  \n",
    "train_folders = maybe_extract(train_filename)\n",
    "test_folders = maybe_extract(test_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-size:larger;\">**Load all images** and create a single Tensor for each letter. For example, there are about 53000 pictures of letter A in the dataset from which we will choose 10000 - the script will create a single Tensor of dimensions (10000, 28, 28), where 28 is both the width and height of each image. Due to memory constraints, we will save each Tensor into a [pickle](https://docs.python.org/3/library/pickle.html).</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_size = 28                             # pixel width and height\n",
    "pixel_depth = 255.0                         # number of levels per pixel\n",
    "\n",
    "max_training_images_per_class = 10000       # maximum number of training images to load per class\n",
    "min_training_images_per_class = 10000       # minimum number of training images to load per class\n",
    "\n",
    "max_testing_images_per_class = 2000         # maximum number of testing images to load per class\n",
    "min_testing_images_per_class = 1800         # minimum number of testing images to load per class\n",
    "\n",
    "def load_letter(folder, max_images, min_images):\n",
    "  \"\"\"Load the data for a single letter label.\"\"\"\n",
    "  image_files = os.listdir(folder)\n",
    "  max_images = min(max_images, len(image_files))\n",
    "  dataset = np.ndarray(shape=(max_images, image_size, image_size),\n",
    "                         dtype=np.float32)\n",
    "\n",
    "  num_images = 0\n",
    "  for image in image_files:\n",
    "    \n",
    "    if num_images >= max_images:\n",
    "      break\n",
    "    \n",
    "    image_file = os.path.join(folder, image)\n",
    "    try:\n",
    "      image_data = (ndimage.imread(image_file).astype(float) - \n",
    "                    pixel_depth / 2) / pixel_depth\n",
    "      if image_data.shape != (image_size, image_size):\n",
    "        raise Exception('Unexpected image shape: %s' % str(image_data.shape))\n",
    "      dataset[num_images, :, :] = image_data\n",
    "      num_images = num_images + 1\n",
    "    except IOError as e:\n",
    "      pass\n",
    "      # print('Could not read:', image_file, ':', e, '- it\\'s ok, skipping.')\n",
    "    \n",
    "  dataset = dataset[0:num_images, :, :]\n",
    "  if num_images < min_images:\n",
    "    raise Exception('Many fewer images than expected: %d < %d' %\n",
    "                    (num_images, min_images))\n",
    "    \n",
    "  print('Full dataset tensor:', dataset.shape)\n",
    "  print()\n",
    "  # print('Mean:', np.mean(dataset))\n",
    "  # print('Standard deviation:', np.std(dataset))\n",
    "  return dataset\n",
    "        \n",
    "def maybe_pickle(data_folders, max_images, min_images, force=False):\n",
    "  dataset_names = []\n",
    "  for folder in data_folders:\n",
    "    set_filename = folder + '.pickle'\n",
    "    dataset_names.append(set_filename)\n",
    "    if os.path.exists(set_filename) and not force:\n",
    "      # You may override by setting force=True.\n",
    "      print('%s already present - Skipping pickling.' % set_filename)\n",
    "    else:\n",
    "      print('Pickling %s.' % set_filename)\n",
    "      dataset = load_letter(folder, max_images, min_images)\n",
    "      try:\n",
    "        with open(set_filename, 'wb') as f:\n",
    "          pickle.dump(dataset, f, pickle.HIGHEST_PROTOCOL)\n",
    "      except Exception as e:\n",
    "        print('Unable to save data to', set_filename, ':', e)\n",
    "  \n",
    "  return dataset_names\n",
    "\n",
    "train_datasets = maybe_pickle(train_folders, max_training_images_per_class, min_training_images_per_class)\n",
    "test_datasets = maybe_pickle(test_folders, max_testing_images_per_class, min_testing_images_per_class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-size:larger;\">Finally, **create a subset** for training, validation and testing, and **save** the preprocessed dataset.</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create training, validation and testing splits\n",
    "def make_arrays(nb_rows, img_size):\n",
    "  if nb_rows:\n",
    "    dataset = np.ndarray((nb_rows, img_size, img_size), dtype=np.float32)\n",
    "    labels = np.ndarray(nb_rows, dtype=np.int32)\n",
    "  else:\n",
    "    dataset, labels = None, None\n",
    "  return dataset, labels\n",
    "\n",
    "def merge_datasets(pickle_files, train_size, valid_size=0):\n",
    "  num_classes = len(pickle_files)\n",
    "  valid_dataset, valid_labels = make_arrays(valid_size, image_size)\n",
    "  train_dataset, train_labels = make_arrays(train_size, image_size)\n",
    "  vsize_per_class = valid_size // num_classes\n",
    "  tsize_per_class = train_size // num_classes\n",
    "    \n",
    "  start_v, start_t = 0, 0\n",
    "  end_v, end_t = vsize_per_class, tsize_per_class\n",
    "  end_l = vsize_per_class+tsize_per_class\n",
    "  for label, pickle_file in enumerate(pickle_files):       \n",
    "    try:\n",
    "      with open(pickle_file, 'rb') as f:\n",
    "        letter_set = pickle.load(f)\n",
    "        # let's shuffle the letters to have random validation and training set\n",
    "        np.random.shuffle(letter_set)\n",
    "        if valid_dataset is not None:\n",
    "          valid_letter = letter_set[:vsize_per_class, :, :]\n",
    "          valid_dataset[start_v:end_v, :, :] = valid_letter\n",
    "          valid_labels[start_v:end_v] = label\n",
    "          start_v += vsize_per_class\n",
    "          end_v += vsize_per_class\n",
    "                    \n",
    "        train_letter = letter_set[vsize_per_class:end_l, :, :]\n",
    "        train_dataset[start_t:end_t, :, :] = train_letter\n",
    "        train_labels[start_t:end_t] = label\n",
    "        start_t += tsize_per_class\n",
    "        end_t += tsize_per_class\n",
    "    except Exception as e:\n",
    "      print('Unable to process data from', pickle_file, ':', e)\n",
    "      raise\n",
    "    \n",
    "  return valid_dataset, valid_labels, train_dataset, train_labels\n",
    "            \n",
    "            \n",
    "train_size = 90000\n",
    "valid_size = 10000\n",
    "test_size = 10000\n",
    "\n",
    "valid_dataset, valid_labels, train_dataset, train_labels = merge_datasets(\n",
    "  train_datasets, train_size, valid_size)\n",
    "_, _, test_dataset, test_labels = merge_datasets(test_datasets, test_size)\n",
    "\n",
    "print('Training:', train_dataset.shape, train_labels.shape)\n",
    "print('Validation:', valid_dataset.shape, valid_labels.shape)\n",
    "print('Testing:', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffle the dataset\n",
    "def randomize(dataset, labels):\n",
    "  permutation = np.random.permutation(labels.shape[0])\n",
    "  shuffled_dataset = dataset[permutation,:,:]\n",
    "  shuffled_labels = labels[permutation]\n",
    "  return shuffled_dataset, shuffled_labels\n",
    "train_dataset, train_labels = randomize(train_dataset, train_labels)\n",
    "test_dataset, test_labels = randomize(test_dataset, test_labels)\n",
    "valid_dataset, valid_labels = randomize(valid_dataset, valid_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the dataset as a pickle\n",
    "pickle_file = os.path.join(data_root, 'notMNIST.pickle')\n",
    "\n",
    "try:\n",
    "  f = open(pickle_file, 'wb')\n",
    "  save = {\n",
    "    'train_dataset': train_dataset,\n",
    "    'train_labels': train_labels,\n",
    "    'valid_dataset': valid_dataset,\n",
    "    'valid_labels': valid_labels,\n",
    "    'test_dataset': test_dataset,\n",
    "    'test_labels': test_labels,\n",
    "    }\n",
    "  pickle.dump(save, f, pickle.HIGHEST_PROTOCOL)\n",
    "  f.close()\n",
    "except Exception as e:\n",
    "  print('Unable to save data to', pickle_file, ':', e)\n",
    "  raise\n",
    "    \n",
    "statinfo = os.stat(pickle_file)\n",
    "print('Compressed pickle size:', statinfo.st_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-size:larger;\">Alternatively, you can download the preprocessed dataset from [this](https://drive.google.com/file/d/0BwaX_s62pOTnUkxlbHh3Zzl4UlU/view?usp=sharing) link and place it in the **data/notMNIST** directory.</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Building a classification model ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-size:larger;\">In this section, you will build a **neural network** for letter classification and train it with **batch gradient descent**, **stochastic gradient descent** and **mini-batch gradient descent**.</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-size:larger;\">You should have the preprocessed subset of notMNIST saved as a [pickle](). You can use the code snippet below to **load** it any time.</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load a subset of the notMNIST dataset\n",
    "data_root = 'data/notMNIST'\n",
    "pickle_file = os.path.join(data_root, 'notMNIST.pickle')\n",
    "\n",
    "with open(pickle_file, 'rb') as f:\n",
    "  save = pickle.load(f)\n",
    "  train_dataset = save['train_dataset']\n",
    "  train_labels = save['train_labels']\n",
    "  valid_dataset = save['valid_dataset']\n",
    "  valid_labels = save['valid_labels']\n",
    "  test_dataset = save['test_dataset']\n",
    "  test_labels = save['test_labels']\n",
    "  del save  # hint to help gc free up memory\n",
    "  print('Training set', train_dataset.shape, train_labels.shape)\n",
    "  print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "  print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-size:larger;\">The dataset stores each image as a Tensor of rank two. However, a fully-connected (standard) neural network only accepts vectors (Tensors of rank 1). Therefore, the following cell **vectorizes each image in the dataset**.</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![flatten image](images/flatten_image.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectorize each image\n",
    "def maybe_vectorize(dataset):\n",
    "  if len(dataset.shape) == 3:\n",
    "    return np.reshape(dataset, (dataset.shape[0], dataset.shape[1] * dataset.shape[2]))\n",
    "  else:\n",
    "    return dataset\n",
    "\n",
    "train_dataset = maybe_vectorize(train_dataset)\n",
    "valid_dataset = maybe_vectorize(valid_dataset)\n",
    "test_dataset = maybe_vectorize(test_dataset)\n",
    "\n",
    "print('Training dataset shape:', train_dataset.shape)\n",
    "print('Validation dataset shape:', valid_dataset.shape)\n",
    "print('Test dataset shape:', test_dataset.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-size:larger;\">Furthermore, the dataset labels (which record what letter is depicted on each image) are stores as integers where 0 represents letter A and 9 represents J. A neural network usually outputs a vector in which each element represents the probability that an input image belongs to a certain label. In order to train the neural network, you will need to compare the predicted probabilities with the correct label. To do this, it's convenient to turn each label into a vector with a one in the position that corresponds to its index and the rest set to zero. This is called **one-hot encoding**.</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![one-hot encoding](images/one_hot_encoding.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one-hot encode each label\n",
    "def maybe_turn_to_one_hot(labels, num_labels=10):\n",
    "  if len(labels.shape) == 1:\n",
    "    one_hot = np.zeros((labels.shape[0], num_labels))\n",
    "    one_hot[np.arange(len(labels)), labels] = 1\n",
    "    return one_hot\n",
    "  else:\n",
    "    return labels\n",
    "\n",
    "train_labels = maybe_turn_to_one_hot(train_labels)\n",
    "valid_labels = maybe_turn_to_one_hot(valid_labels)\n",
    "test_labels = maybe_turn_to_one_hot(test_labels)\n",
    "\n",
    "print('Training labels shape:', train_labels.shape)\n",
    "print('Validation labels shape:', valid_labels.shape)\n",
    "print('Test labels shape:', test_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fully-connected Neural Network ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-size:larger;\">In the following series of tasks, you will implement a classfication model from scratch. We recommend you to use the Jupyter notebook from the first tutorial as a reference and try to implement your model based on that. Alternatively, you check the reference notebook which has all the solution (except for Part 4 which contains a bonus-point task) but you won't learn much by copying them. The tasks aren't graded.</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1A ###\n",
    "\n",
    "* <span style=\"font-size:larger;\">define a computation graph for a **fully-connected neural network**</span>\n",
    "* <span style=\"font-size:larger;\">the network should have **2 hidden layers** with **200 neurons in the first** and **100 neurons in the second** hidden layer</span>\n",
    "* <span style=\"font-size:larger;\">**input**: vectorized images in the shape (num_images, 784)</span>\n",
    "* <span style=\"font-size:larger;\">**output**: predictions in the shape (num_images, 10)</span>\n",
    "\n",
    "<span style=\"font-size:larger;\">See the reference notebook for a solution.</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1B ###\n",
    "\n",
    "* <span style=\"font-size:larger;\">train the neural network you defined above using **Batch Gradient Descent**</span>\n",
    "* <span style=\"font-size:larger;\">during each learning step, the network makes predictions for all 90000 training images and learns from its mistakes</span>\n",
    "* <span style=\"font-size:larger;\">evaluate the network on the notMNIST evaluation set</span>\n",
    "\n",
    "\n",
    "* <span style=\"font-size:larger;\">recommended learning rate: 0.1</span>\n",
    "* <span style=\"font-size:larger;\">recommended number of training steps: 60</span>\n",
    "\n",
    "<span style=\"font-size:larger;\">See the reference notebook for a solution.</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1C ###\n",
    "\n",
    "* <span style=\"font-size:larger;\">train the neural network you defined above using **Stochastic Gradient Descent**</span>\n",
    "* <span style=\"font-size:larger;\">during each learning step, the network makes predictions for a single image and learns from its mistakes</span>\n",
    "* <span style=\"font-size:larger;\">evaluate the network on the notMNIST evaluation set</span>\n",
    "\n",
    "\n",
    "* <span style=\"font-size:larger;\">recommended learning rate: 0.01</span>\n",
    "* <span style=\"font-size:larger;\">recommended number of training steps: 20000</span>\n",
    "\n",
    "<span style=\"font-size:larger;\">See the reference notebook for a solution.</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1D ###\n",
    "\n",
    "* <span style=\"font-size:larger;\">train the neural network you defined above using **Mini-batch Gradient Descent**</span>\n",
    "* <span style=\"font-size:larger;\">during each learning step, the network makes predictions for a small batch of images and learns from its mistakes</span>\n",
    "* <span style=\"font-size:larger;\">evaluate the network on the notMNIST evaluation set</span>\n",
    "\n",
    "\n",
    "* <span style=\"font-size:larger;\">recommended mini-batch size: 64</span>\n",
    "* <span style=\"font-size:larger;\">recommended learning rate: 0.05</span>\n",
    "* <span style=\"font-size:larger;\">recommended number of training steps: 5000</span>\n",
    "\n",
    "<span style=\"font-size:larger;\">See the reference notebook a the solution.</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Saving models and visualizing learning in Tensorflow ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-size:larger;\">It isn't very convenient to train your model each time you want to use it. On top of that, more complex Computer Vision models take weeks to train.</span>\n",
    "\n",
    "<span style=\"font-size:larger;\">In this section, you will learn how to save and load a Tensorflow model. In addition, you will visualize how the loss and accuracy changes during the training of your neural network using Tensorboard.</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2A ###\n",
    "\n",
    "<span style=\"font-size:larger;\">Copy the definition of your neural network and add the following lines to the end of the code snippet. You might need to change the names of the variables or delete the second line if you haven't defined an operation that measures the accuracy of your model.</span>\n",
    "\n",
    "```\n",
    "<span style=\"font-size:larger;\">tf.summary.scalar('loss', loss)</span>\n",
    "<span style=\"font-size:larger;\">tf.summary.scalar('accuracy', accuracy)</span>\n",
    "<span style=\"font-size:larger;\">summaries = tf.summary.merge_all()</span>\n",
    "```\n",
    "\n",
    "<span style=\"font-size:larger;\">The first two lines create summary operations for your model's loss and accuracy which will be recorded during each training step. The third line groups the two summaries together so that you have a single operation that is easy to work with.</span>\n",
    "\n",
    "<span style=\"font-size:larger;\">See the reference notebook a the solution.</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2B ###\n",
    "\n",
    "<span style=\"font-size:larger;\">Add these lines to the beginning of your trainig script.</span>\n",
    "\n",
    "```\n",
    "saver = tf.train.Saver()\n",
    "summary_writer = tf.summary.FileWriter('data', graph=tf.get_default_graph())\n",
    "```\n",
    "\n",
    "<span style=\"font-size:larger;\">You can save a summary using `summary_writer.add_summary(summary, global_step=step)` and the model using `saver.save(session, os.path.join('data/notMNIST-model-1'), global_step=step)`.</span>\n",
    "\n",
    "<span style=\"font-size:larger;\">See the reference notebook a the solution.</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-size:larger;\">You can **load** and evaluate your model using the following script. When saving a model, Tensorflow generates three different files (data, meta and index). To load a model, simply specify its path without the extension.</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_your_model = None\n",
    "\n",
    "if path_to_your_model is None:\n",
    "  print(\"Please specify the path to your saved model.\")\n",
    "else:\n",
    "  saver = tf.train.Saver()\n",
    "\n",
    "  with tf.Session() as session:\n",
    "    session.run(tf.global_variables_initializer())\n",
    "    saver.restore(session, path_to_your_model)\n",
    "\n",
    "    validation_accuracy = session.run(accuracy, feed_dict={\n",
    "      input_data: valid_dataset,\n",
    "      input_labels: valid_labels\n",
    "    })\n",
    "    print('Validation accuracy:', validation_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-size:larger;\">To visualize the training, call the following command.</span>\n",
    "\n",
    "```\n",
    "tensorboard --logdir data\n",
    "```\n",
    "\n",
    "<span style=\"font-size:larger;\">You must activate the virtual environment that contains your instalation of Tensorflow before you can call this command.</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Regularization ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-size:larger;\">As you might have noticed, some of the models you have trained earlier report training accuracy that is much higher than the validation or testing accuracy. This is due to the model **overfitting** on the training data. Overfitting can be mitigated using **regularization** techniques.</span>\n",
    "\n",
    "<span style=\"font-size:larger;\">**Dropout** is a popular technique that prevents overfitting by dropping some of the activation of a particular layer. Take a look at [dropout in Tensorflow](https://www.tensorflow.org/api_docs/python/tf/layers/dropout). You can add it into your computation graph as a layer similarly to the Dense layer.</span> \n",
    "\n",
    "<span style=\"font-size:larger;\">However, there is one more thing you need to do before you can use it. Dropout should drop the activations only during training because we don't want to loose any information when we use the model. You will need to define a boolean Tensor that will tell the dropout layer if it's in the training or testing mode.</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3 (bonus points) ###\n",
    "\n",
    "<span style=\"font-size:larger;\">Add a dropout layer **before** the last Dense layer and make sure that activations are dropped only during training. Try changing the drop probability in order to obtain the best validation accuracy. Use the neural network you have defined above and train it with mini-batch gradient descent.</span>\n",
    "\n",
    "<span style=\"font-size:larger;\">The solution will be added to the reference notebook after the end of this tutorial.</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: computation graph definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Optional) Part 5: Convolutional Neural Networks ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-size:larger;\">Fully-connected neural network are not appropriate for modelling images becuase they aren't invariant to translations and have too many weights. For these reasons, a different type of neural network was developed. Convolutional Neural Networks (ConvNets) use filters and max-pooling layers to keep the number of weights low and to learn to recognize object regardless of their position in the image. Moreover, they are easy to implement in Tensorflow</span>\n",
    "\n",
    "<span style=\"font-size:larger;\">Implement a simple ConvNet with convolutional, max-pooling and dense layers.</span>\n",
    "\n",
    "<span style=\"font-size:larger;\">Useful links:</span>\n",
    "* [lecture notes Stanford University **(recommended)**](http://cs231n.github.io/convolutional-networks/)\n",
    "* [Tensorflow tutorial](https://www.tensorflow.org/tutorials/layers)\n",
    "\n",
    "<span style=\"font-size:larger;\">See the reference notebook for a solution.</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load a subset of the notMNIST dataset\n",
    "pickle_file = os.path.join(data_root, 'notMNIST.pickle')\n",
    "\n",
    "with open(pickle_file, 'rb') as f:\n",
    "  save = pickle.load(f)\n",
    "  train_dataset = save['train_dataset']\n",
    "  train_labels = save['train_labels']\n",
    "  valid_dataset = save['valid_dataset']\n",
    "  valid_labels = save['valid_labels']\n",
    "  test_dataset = save['test_dataset']\n",
    "  test_labels = save['test_labels']\n",
    "  del save  # hint to help gc free up memory\n",
    "  print('Training set', train_dataset.shape, train_labels.shape)\n",
    "  print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "  print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def maybe_add_channels_dimension(dataset):\n",
    "  if len(dataset.shape) == 3:\n",
    "    return np.expand_dims(dataset, axis=-1)\n",
    "  else:\n",
    "    return dataset\n",
    "\n",
    "train_dataset = maybe_add_channels_dimension(train_dataset)\n",
    "valid_dataset = maybe_add_channels_dimension(valid_dataset)\n",
    "test_dataset = maybe_add_channels_dimension(test_dataset)\n",
    "\n",
    "print('Training dataset shape:', train_dataset.shape)\n",
    "print('Validation dataset shape:', valid_dataset.shape)\n",
    "print('Test dataset shape:', test_dataset.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def maybe_turn_to_one_hot(labels, num_labels=10):\n",
    "  if len(labels.shape) == 1:\n",
    "    one_hot = np.zeros((labels.shape[0], num_labels))\n",
    "    one_hot[np.arange(len(labels)), labels] = 1\n",
    "    return one_hot\n",
    "  else:\n",
    "    return labels\n",
    "\n",
    "train_labels = maybe_turn_to_one_hot(train_labels)\n",
    "valid_labels = maybe_turn_to_one_hot(valid_labels)\n",
    "test_labels = maybe_turn_to_one_hot(test_labels)\n",
    "\n",
    "print('Training labels shape:', train_labels.shape)\n",
    "print('Validation labels shape:', valid_labels.shape)\n",
    "print('Test labels shape:', test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: computation graph definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Optional) Part 6: Regularizing ConvNets ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-size:larger;\">All neural networks are prone to overfitting if the training dataset is too small. Implement dropout for your ConvNet. See the reference notebook for a solution.</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: computation graph definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Resources   ##\n",
    "\n",
    "** Saving and restoring models in Tensorfow **\n",
    "* [tutorial](https://www.tensorflow.org/programmers_guide/saved_model)\n",
    "\n",
    "** Visualizing learning using Tensorboard **\n",
    "* [tutorial](https://www.tensorflow.org/get_started/summaries_and_tensorboard)\n",
    "\n",
    "** Convolutional Networks **\n",
    "* [lecture notes Stanford University **(recommended)**](http://cs231n.github.io/convolutional-networks/)\n",
    "* [lecture video from the University of Oxford](https://www.youtube.com/watch?v=bEUX_56Lojc)\n",
    "* [Tensorflow tutorial](https://www.tensorflow.org/tutorials/layers)\n",
    "\n",
    "** Dropout **\n",
    "* [documentation](https://www.tensorflow.org/api_docs/python/tf/layers/dropout)\n",
    "* [paper](https://www.cs.toronto.edu/~hinton/absps/JMLRdropout.pdf)\n",
    "\n",
    "** Advanced data loading features in Tensorflow **\n",
    "* [tutorial](https://www.tensorflow.org/programmers_guide/datasets)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try out different dataset ##\n",
    "\n",
    "* [Dogs vs. Cats](https://www.kaggle.com/c/dogs-vs-cats)\n",
    "* [CIFAR-10](https://www.kaggle.com/c/cifar-10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
