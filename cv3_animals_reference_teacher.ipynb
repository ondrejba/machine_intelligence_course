{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# MI-MVI tutorial 2 #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-size:larger;\">In the first tutorial, we introduced you to **Tensorflow**, a Deep Learning framework. You learned how to **define a computation graph**, and **create and initialize Sessions**. Finally, you trained a simple classification model on a dataset of hand-written digits called **MNIST**.</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-size:larger;\">In this tutorial, you will download and preprocess a new dataset from scratch. Furthermore, we will show you how to use some advanced Tensorflow features like saving and loading models as well as visualizing the training. Lastly, you will experiment with various neural network architectures to obtain a good classifier.</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![animals](images/animals.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-size:larger;\">We create a small dataset of **animals**. From top to bottom: cat, deer, dog and horse. The task is to train a Neural Network to distinguish between the four classes.</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-size:larger;\">**Import** packages we will need.</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, pickle\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-size:larger;\">**Load** pictures of cars and airplanes. The pictures are already prepared for you as [numpy arrays](https://docs.scipy.org/doc/numpy-dev/user/quickstart.html).</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = \"data/animals/dataset.pickle\"\n",
    "    \n",
    "with open(dataset_path, \"rb\") as file:\n",
    "    dataset = pickle.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-size:larger;\">We will work with 8000 training, 1000 validation and 1000 testing pictures. All pictures contain color and span 32 x 32 pixels.</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train airplanes: (8000, 32, 32, 3)\n",
      "valid airplanes: (1000, 32, 32, 3)\n",
      "test airplanes: (1000, 32, 32, 3)\n"
     ]
    }
   ],
   "source": [
    "print(\"train airplanes:\", dataset[\"train_data\"].shape)\n",
    "print(\"valid airplanes:\", dataset[\"valid_data\"].shape)\n",
    "print(\"test airplanes:\", dataset[\"valid_data\"].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-size:larger;\">**Shuffle** the pictures.</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/4601373/better-way-to-shuffle-two-numpy-arrays-in-unison\n",
    "def unison_shuffle(a, b):\n",
    "    assert len(a) == len(b)\n",
    "    p = np.random.permutation(len(a))\n",
    "    return a[p], b[p]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, train_labels = unison_shuffle(dataset[\"train_data\"], dataset[\"train_labels\"])\n",
    "valid_dataset, valid_labels = unison_shuffle(dataset[\"valid_data\"], dataset[\"valid_labels\"])\n",
    "test_dataset, test_labels = unison_shuffle(dataset[\"valid_data\"], dataset[\"test_labels\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Building a classification model ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-size:larger;\">In this section, you will build a **neural network** for letter classification and train it with **batch gradient descent**, **stochastic gradient descent** and **mini-batch gradient descent**.</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-size:larger;\">The dataset stores each image as a Tensor of rank two. However, a fully-connected (standard) neural network only accepts vectors (Tensors of rank 1). Therefore, the following cell **vectorizes each image in the dataset**.</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![flatten image](images/flatten_image.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = np.mean(train_dataset, axis=-1)\n",
    "valid_dataset = np.mean(valid_dataset, axis=-1)\n",
    "test_dataset = np.mean(test_dataset, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training dataset shape: (8000, 1024)\n",
      "Validation dataset shape: (1000, 1024)\n",
      "Test dataset shape: (1000, 1024)\n"
     ]
    }
   ],
   "source": [
    "# vectorize each image\n",
    "def maybe_vectorize(dataset):\n",
    "  if len(dataset.shape) == 3:\n",
    "    return np.reshape(dataset, (dataset.shape[0], dataset.shape[1] * dataset.shape[2]))\n",
    "  else:\n",
    "    return dataset\n",
    "\n",
    "train_dataset = maybe_vectorize(train_dataset)\n",
    "valid_dataset = maybe_vectorize(valid_dataset)\n",
    "test_dataset = maybe_vectorize(test_dataset)\n",
    "\n",
    "print('Training dataset shape:', train_dataset.shape)\n",
    "print('Validation dataset shape:', valid_dataset.shape)\n",
    "print('Test dataset shape:', test_dataset.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-size:larger;\">Furthermore, the dataset labels (which record what letter is depicted on each image) are stores as integers where 0 represents letter A and 9 represents J. A neural network usually outputs a vector in which each element represents the probability that an input image belongs to a certain label. In order to train the neural network, you will need to compare the predicted probabilities with the correct label. To do this, it's convenient to turn each label into a vector with a one in the position that corresponds to its index and the rest set to zero. This is called **one-hot encoding**.</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![one-hot encoding](images/one_hot_encoding.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training labels shape: (8000, 4)\n",
      "Validation labels shape: (1000, 4)\n",
      "Test labels shape: (1000, 4)\n"
     ]
    }
   ],
   "source": [
    "# one-hot encode each label\n",
    "def maybe_turn_to_one_hot(labels, num_labels=4):\n",
    "  if len(labels.shape) == 1:\n",
    "    one_hot = np.zeros((labels.shape[0], num_labels))\n",
    "    one_hot[np.arange(len(labels)), labels] = 1\n",
    "    return one_hot\n",
    "  else:\n",
    "    return labels\n",
    "\n",
    "train_labels = maybe_turn_to_one_hot(train_labels)\n",
    "valid_labels = maybe_turn_to_one_hot(valid_labels)\n",
    "test_labels = maybe_turn_to_one_hot(test_labels)\n",
    "\n",
    "print('Training labels shape:', train_labels.shape)\n",
    "print('Validation labels shape:', valid_labels.shape)\n",
    "print('Test labels shape:', test_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fully-connected Neural Network ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-size:larger;\">In the following series of tasks, you will implement a classfication model from scratch. We recommend you to use the Jupyter notebook from the first tutorial as a reference and try to implement your model based on that. Alternatively, you check the reference notebook which has all the solution (except for Part 4 which contains a bonus-point task) but you won't learn much by copying them. The tasks aren't graded.</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1A ###\n",
    "\n",
    "* <span style=\"font-size:larger;\">define a computation graph for a **fully-connected neural network**</span>\n",
    "* <span style=\"font-size:larger;\">the network should have **2 hidden layers** with **200 neurons in the first** and **100 neurons in the second** hidden layer</span>\n",
    "* <span style=\"font-size:larger;\">**input**: vectorized images in the shape (num_images, 784)</span>\n",
    "* <span style=\"font-size:larger;\">**output**: predictions in the shape (num_images, 10)</span>\n",
    "\n",
    "<span style=\"font-size:larger;\">See the reference notebook for a solution.</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.reset_default_graph()   # TF remembers everything you defined, this will keep the computation graph clean\n",
    "\n",
    "#learning_rate = 0.1              # Batch Gradient Descent\n",
    "#learning_rate = 0.01              # Stochastic Gradient Descent\n",
    "learning_rate = 0.05             # Mini-batch Gradient Descent\n",
    "\n",
    "input_data = tf.placeholder(tf.float32, (None, train_dataset.shape[1]))\n",
    "input_labels = tf.placeholder(tf.int32, (None, train_labels.shape[1]))\n",
    "\n",
    "layer1 = tf.layers.dense(input_data / 255, 200, activation=tf.nn.relu)\n",
    "layer2 = tf.layers.dense(layer1, 100, activation=tf.nn.relu)\n",
    "logits = tf.layers.dense(layer2, 4)\n",
    "\n",
    "batch_loss = tf.nn.softmax_cross_entropy_with_logits(labels=input_labels, logits=logits)\n",
    "loss = tf.reduce_mean(batch_loss)\n",
    "train_op = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)\n",
    "\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(tf.argmax(logits, 1), tf.argmax(input_labels, 1)), tf.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1B ###\n",
    "\n",
    "* <span style=\"font-size:larger;\">train the neural network you defined above using **Batch Gradient Descent**</span>\n",
    "* <span style=\"font-size:larger;\">during each learning step, the network makes predictions for all 90000 training images and learns from its mistakes</span>\n",
    "* <span style=\"font-size:larger;\">evaluate the network on the notMNIST evaluation set</span>\n",
    "\n",
    "\n",
    "* <span style=\"font-size:larger;\">recommended learning rate: 0.1</span>\n",
    "* <span style=\"font-size:larger;\">recommended number of training steps: 60</span>\n",
    "\n",
    "<span style=\"font-size:larger;\">See the reference notebook for a solution.</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 0 , loss: 1.502508 , training accuracy: 0.2485\n",
      "step: 1 , loss: 1.5470589 , training accuracy: 0.237875\n",
      "step: 2 , loss: 1.3833781 , training accuracy: 0.282375\n",
      "step: 3 , loss: 1.373744 , training accuracy: 0.327875\n",
      "step: 4 , loss: 1.3673692 , training accuracy: 0.314375\n",
      "step: 5 , loss: 1.3626182 , training accuracy: 0.3485\n",
      "step: 6 , loss: 1.3581783 , training accuracy: 0.319\n",
      "step: 7 , loss: 1.3562946 , training accuracy: 0.357\n",
      "step: 8 , loss: 1.351899 , training accuracy: 0.31975\n",
      "step: 9 , loss: 1.3519707 , training accuracy: 0.360125\n",
      "step: 10 , loss: 1.3451397 , training accuracy: 0.322875\n",
      "step: 11 , loss: 1.3433777 , training accuracy: 0.37825\n",
      "step: 12 , loss: 1.3372444 , training accuracy: 0.329375\n",
      "step: 13 , loss: 1.3349733 , training accuracy: 0.39175\n",
      "step: 14 , loss: 1.3312559 , training accuracy: 0.33575\n",
      "step: 15 , loss: 1.3307893 , training accuracy: 0.391875\n",
      "step: 16 , loss: 1.3299668 , training accuracy: 0.34\n",
      "step: 17 , loss: 1.3332673 , training accuracy: 0.375875\n",
      "step: 18 , loss: 1.33684 , training accuracy: 0.3425\n",
      "step: 19 , loss: 1.3444958 , training accuracy: 0.350625\n",
      "step: 20 , loss: 1.3359448 , training accuracy: 0.342125\n",
      "step: 21 , loss: 1.3345493 , training accuracy: 0.37375\n",
      "step: 22 , loss: 1.3219539 , training accuracy: 0.354125\n",
      "step: 23 , loss: 1.3196753 , training accuracy: 0.382125\n",
      "step: 24 , loss: 1.3142915 , training accuracy: 0.382375\n",
      "step: 25 , loss: 1.3137927 , training accuracy: 0.380875\n",
      "step: 26 , loss: 1.3121797 , training accuracy: 0.392125\n",
      "step: 27 , loss: 1.3157166 , training accuracy: 0.370375\n",
      "step: 28 , loss: 1.3187656 , training accuracy: 0.381375\n",
      "step: 29 , loss: 1.3223546 , training accuracy: 0.35325\n",
      "step: 30 , loss: 1.322978 , training accuracy: 0.3875\n",
      "step: 31 , loss: 1.31115 , training accuracy: 0.358\n",
      "step: 32 , loss: 1.3048948 , training accuracy: 0.405\n",
      "step: 33 , loss: 1.3002421 , training accuracy: 0.378875\n",
      "step: 34 , loss: 1.3036523 , training accuracy: 0.4025\n",
      "step: 35 , loss: 1.2992969 , training accuracy: 0.374875\n",
      "step: 36 , loss: 1.3057262 , training accuracy: 0.39975\n",
      "step: 37 , loss: 1.3018228 , training accuracy: 0.368375\n",
      "step: 38 , loss: 1.307314 , training accuracy: 0.39125\n",
      "step: 39 , loss: 1.3068334 , training accuracy: 0.359125\n",
      "step: 40 , loss: 1.3111451 , training accuracy: 0.389875\n",
      "step: 41 , loss: 1.3002015 , training accuracy: 0.35725\n",
      "step: 42 , loss: 1.2974974 , training accuracy: 0.41225\n",
      "step: 43 , loss: 1.2885118 , training accuracy: 0.377375\n",
      "step: 44 , loss: 1.2887855 , training accuracy: 0.424625\n",
      "step: 45 , loss: 1.2852896 , training accuracy: 0.39725\n",
      "step: 46 , loss: 1.2910278 , training accuracy: 0.406625\n",
      "step: 47 , loss: 1.2944108 , training accuracy: 0.404625\n",
      "step: 48 , loss: 1.3006666 , training accuracy: 0.366875\n",
      "step: 49 , loss: 1.3021692 , training accuracy: 0.408375\n",
      "step: 50 , loss: 1.2878977 , training accuracy: 0.383125\n",
      "step: 51 , loss: 1.2837334 , training accuracy: 0.419375\n",
      "step: 52 , loss: 1.2792294 , training accuracy: 0.390125\n",
      "step: 53 , loss: 1.2874238 , training accuracy: 0.408875\n",
      "step: 54 , loss: 1.2804893 , training accuracy: 0.37675\n",
      "step: 55 , loss: 1.2883222 , training accuracy: 0.409\n",
      "step: 56 , loss: 1.2739633 , training accuracy: 0.391\n",
      "step: 57 , loss: 1.275304 , training accuracy: 0.419125\n",
      "step: 58 , loss: 1.2701075 , training accuracy: 0.398875\n",
      "step: 59 , loss: 1.2752054 , training accuracy: 0.41675\n",
      "Training finished after 60 steps.\n",
      "Validation accuracy 0.37 .\n"
     ]
    }
   ],
   "source": [
    "num_steps = 60\n",
    "\n",
    "with tf.Session() as session:\n",
    "  session.run(tf.global_variables_initializer())\n",
    "\n",
    "  for step in range(num_steps):\n",
    "    batch_loss, batch_accuracy, _ = session.run([loss, accuracy, train_op], feed_dict={\n",
    "      input_data: train_dataset,\n",
    "      input_labels: train_labels\n",
    "    })\n",
    "    print('step:', step, ', loss:', batch_loss, ', training accuracy:', batch_accuracy)\n",
    "    \n",
    "  print('Training finished after', num_steps, 'steps.')\n",
    "  validation_accuracy = session.run(accuracy, feed_dict={\n",
    "    input_data: valid_dataset,\n",
    "    input_labels: valid_labels\n",
    "  })\n",
    "  print('Validation accuracy', validation_accuracy, '.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1C ###\n",
    "\n",
    "* <span style=\"font-size:larger;\">train the neural network you defined above using **Stochastic Gradient Descent**</span>\n",
    "* <span style=\"font-size:larger;\">during each learning step, the network makes predictions for a single image and learns from its mistakes</span>\n",
    "* <span style=\"font-size:larger;\">evaluate the network on the notMNIST evaluation set</span>\n",
    "\n",
    "\n",
    "* <span style=\"font-size:larger;\">recommended learning rate: 0.01</span>\n",
    "* <span style=\"font-size:larger;\">recommended number of training steps: 20000</span>\n",
    "\n",
    "<span style=\"font-size:larger;\">See the reference notebook for a solution.</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 0 , loss: 1.6357461 , training accuracy: 0.0\n",
      "step: 1000 , loss: 1.5027313 , training accuracy: 0.0\n",
      "step: 2000 , loss: 1.3430177 , training accuracy: 0.0\n",
      "step: 3000 , loss: 1.4384973 , training accuracy: 0.0\n",
      "step: 4000 , loss: 1.4315618 , training accuracy: 0.0\n",
      "step: 5000 , loss: 1.4247622 , training accuracy: 0.0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-55-1ddbbbb4a975>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m     batch_loss, batch_accuracy, _ = session.run([loss, accuracy, train_op], feed_dict={\n\u001b[1;32m     11\u001b[0m       \u001b[0minput_data\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m       \u001b[0minput_labels\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtrain_labels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m     })\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/envs/mvi/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    893\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 895\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    896\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/envs/mvi/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1126\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1127\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1128\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1129\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1130\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/envs/mvi/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1342\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1343\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1344\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1345\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1346\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/envs/mvi/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1348\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1349\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1350\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1351\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1352\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/envs/mvi/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1327\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1328\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1329\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1330\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1331\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_steps = 20000\n",
    "log_frequency = 1000\n",
    "\n",
    "with tf.Session() as session:\n",
    "  session.run(tf.global_variables_initializer())\n",
    "\n",
    "  for step in range(num_steps):\n",
    "    index = step % train_dataset.shape[0] \n",
    "    \n",
    "    batch_loss, batch_accuracy, _ = session.run([loss, accuracy, train_op], feed_dict={\n",
    "      input_data: train_dataset[index : index + 1],\n",
    "      input_labels: train_labels[index : index + 1]\n",
    "    })\n",
    "    \n",
    "    if step % log_frequency == 0:\n",
    "      print('step:', step, ', loss:', batch_loss, ', training accuracy:', batch_accuracy)\n",
    "    \n",
    "  print('Training finished after', num_steps, 'steps.')\n",
    "  validation_accuracy = session.run(accuracy, feed_dict={\n",
    "    input_data: valid_dataset,\n",
    "    input_labels: valid_labels\n",
    "  })\n",
    "  print('Validation accuracy', validation_accuracy, '.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1D ###\n",
    "\n",
    "* <span style=\"font-size:larger;\">train the neural network you defined above using **Mini-batch Gradient Descent**</span>\n",
    "* <span style=\"font-size:larger;\">during each learning step, the network makes predictions for a small batch of images and learns from its mistakes</span>\n",
    "* <span style=\"font-size:larger;\">evaluate the network on the notMNIST evaluation set</span>\n",
    "\n",
    "\n",
    "* <span style=\"font-size:larger;\">recommended mini-batch size: 64</span>\n",
    "* <span style=\"font-size:larger;\">recommended learning rate: 0.05</span>\n",
    "* <span style=\"font-size:larger;\">recommended number of training steps: 5000</span>\n",
    "\n",
    "<span style=\"font-size:larger;\">See the reference notebook a the solution.</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 0 , loss: 1.5325315 , training accuracy: 0.25\n",
      "step: 500 , loss: 0.78410023 , training accuracy: 0.75\n",
      "step: 1000 , loss: 0.39410794 , training accuracy: 0.890625\n",
      "step: 1500 , loss: 0.7149308 , training accuracy: 0.703125\n",
      "step: 2000 , loss: 0.6830551 , training accuracy: 0.734375\n",
      "step: 2500 , loss: 0.42496115 , training accuracy: 0.84375\n",
      "step: 3000 , loss: 0.3554579 , training accuracy: 0.921875\n",
      "step: 3500 , loss: 0.4985964 , training accuracy: 0.84375\n",
      "step: 4000 , loss: 0.34191248 , training accuracy: 0.859375\n",
      "step: 4500 , loss: 0.65498257 , training accuracy: 0.78125\n",
      "Training finished after 5000 steps.\n",
      "Validation accuracy 0.405 .\n"
     ]
    }
   ],
   "source": [
    "num_steps = 5000\n",
    "mini_batch_size = 64\n",
    "log_frequency = 500\n",
    "\n",
    "with tf.Session() as session:\n",
    "  session.run(tf.global_variables_initializer())\n",
    "\n",
    "  for step in range(num_steps):\n",
    "    start = step\n",
    "    end = step + mini_batch_size\n",
    "    \n",
    "    batch_loss, batch_accuracy, _ = session.run([loss, accuracy, train_op], feed_dict={\n",
    "      input_data: train_dataset.take(range(start, end), axis=0, mode=\"wrap\"),\n",
    "      input_labels: train_labels.take(range(start, end), axis=0, mode=\"wrap\")\n",
    "    })\n",
    "    \n",
    "    if step % log_frequency == 0:\n",
    "      print('step:', step, ', loss:', batch_loss, ', training accuracy:', batch_accuracy)\n",
    "    \n",
    "  print('Training finished after', num_steps, 'steps.')\n",
    "  validation_accuracy = session.run(accuracy, feed_dict={\n",
    "    input_data: valid_dataset,\n",
    "    input_labels: valid_labels\n",
    "  })\n",
    "  print('Validation accuracy', validation_accuracy, '.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Saving models and visualizing learning in Tensorflow ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-size:larger;\">It isn't very convenient to train your model each time you want to use it. On top of that, more complex Computer Vision models take weeks to train.</span>\n",
    "\n",
    "<span style=\"font-size:larger;\">In this section, you will learn how to save and load a Tensorflow model. In addition, you will visualize how the loss and accuracy changes during the training of your neural network using Tensorboard.</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2A ###\n",
    "\n",
    "<span style=\"font-size:larger;\">Copy the definition of your neural network and add the following lines to the end of the code snippet. You might need to change the names of the variables or delete the second line if you haven't defined an operation that measures the accuracy of your model.</span>\n",
    "\n",
    "```\n",
    "<span style=\"font-size:larger;\">tf.summary.scalar('loss', loss)</span>\n",
    "<span style=\"font-size:larger;\">tf.summary.scalar('accuracy', accuracy)</span>\n",
    "<span style=\"font-size:larger;\">summaries = tf.summary.merge_all()</span>\n",
    "```\n",
    "\n",
    "<span style=\"font-size:larger;\">The first two lines create summary operations for your model's loss and accuracy which will be recorded during each training step. The third line groups the two summaries together so that you have a single operation that is easy to work with.</span>\n",
    "\n",
    "<span style=\"font-size:larger;\">See the reference notebook a the solution.</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.reset_default_graph()   # TF remembers everything you defined, this will keep the computation graph clean\n",
    "\n",
    "learning_rate = 0.1\n",
    "\n",
    "input_data = tf.placeholder(tf.float32, (None, train_dataset.shape[1]))\n",
    "input_labels = tf.placeholder(tf.int32, (None, train_labels.shape[1]))\n",
    "\n",
    "layer1 = tf.layers.dense(input_data, 200, activation=tf.nn.relu)\n",
    "layer2 = tf.layers.dense(layer1, 100, activation=tf.nn.relu)\n",
    "logits = tf.layers.dense(layer2, 10)\n",
    "\n",
    "batch_loss = tf.nn.softmax_cross_entropy_with_logits(labels=input_labels, logits=logits)\n",
    "loss = tf.reduce_mean(batch_loss)\n",
    "train_op = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)\n",
    "\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(tf.argmax(logits, 1), tf.argmax(input_labels, 1)), tf.float32))\n",
    "\n",
    "tf.summary.scalar('loss', loss)\n",
    "tf.summary.scalar('accuracy', accuracy)\n",
    "summaries = tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2B ###\n",
    "\n",
    "<span style=\"font-size:larger;\">Add these lines to the beginning of your trainig script.</span>\n",
    "\n",
    "```\n",
    "saver = tf.train.Saver()\n",
    "summary_writer = tf.summary.FileWriter('data', graph=tf.get_default_graph())\n",
    "```\n",
    "\n",
    "<span style=\"font-size:larger;\">You can save a summary using `summary_writer.add_summary(summary, global_step=step)` and the model using `saver.save(session, os.path.join('data/notMNIST-model-1'), global_step=step)`.</span>\n",
    "\n",
    "<span style=\"font-size:larger;\">See the reference notebook a the solution.</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_steps = 10000\n",
    "mini_batch_size = 64\n",
    "log_frequency = 500\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "summary_writer = tf.summary.FileWriter('data', graph=tf.get_default_graph())\n",
    "\n",
    "with tf.Session() as session:\n",
    "  session.run(tf.global_variables_initializer())\n",
    "\n",
    "  for step in range(num_steps):\n",
    "    start = step\n",
    "    end = step + mini_batch_size\n",
    "    \n",
    "    batch_loss, batch_accuracy, summary, _ = session.run([loss, accuracy, summaries, train_op], feed_dict={\n",
    "      input_data: train_dataset.take(range(start, end), axis=0, mode=\"wrap\"),\n",
    "      input_labels: train_labels.take(range(start, end), axis=0, mode=\"wrap\")\n",
    "    })\n",
    "    summary_writer.add_summary(summary, global_step=step)\n",
    "    \n",
    "    if step % log_frequency == 0:\n",
    "      print('step:', step, ', loss:', batch_loss, ', training accuracy:', batch_accuracy)\n",
    "    \n",
    "  print('Training finished after', num_steps, 'steps.')\n",
    "  validation_accuracy = session.run(accuracy, feed_dict={\n",
    "    input_data: valid_dataset,\n",
    "    input_labels: valid_labels\n",
    "  })\n",
    "  print('Validation accuracy', validation_accuracy, '.')\n",
    "\n",
    "  saver.save(session, os.path.join('data/notMNIST-model-1'), global_step=step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-size:larger;\">You can **load** and evaluate your model using the following script. When saving a model, Tensorflow generates three different files (data, meta and index). To load a model, simply specify its path without the extension.</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_your_model = None\n",
    "\n",
    "if path_to_your_model is None:\n",
    "  print(\"Please specify the path to your saved model.\")\n",
    "else:\n",
    "  saver = tf.train.Saver()\n",
    "\n",
    "  with tf.Session() as session:\n",
    "    session.run(tf.global_variables_initializer())\n",
    "    saver.restore(session, path_to_your_model)\n",
    "\n",
    "    validation_accuracy = session.run(accuracy, feed_dict={\n",
    "      input_data: valid_dataset,\n",
    "      input_labels: valid_labels\n",
    "    })\n",
    "    print('Validation accuracy:', validation_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-size:larger;\">To visualize the training, call the following command.</span>\n",
    "\n",
    "```\n",
    "tensorboard --logdir data\n",
    "```\n",
    "\n",
    "<span style=\"font-size:larger;\">You must activate the virtual environment that contains your instalation of Tensorflow before you can call this command.</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Regularization ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-size:larger;\">As you might have noticed, some of the models you have trained earlier report training accuracy that is much higher than the validation or testing accuracy. This is due to the model **overfitting** on the training data. Overfitting can be mitigated using **regularization** techniques.</span>\n",
    "\n",
    "<span style=\"font-size:larger;\">**Dropout** is a popular technique that prevents overfitting by dropping some of the activation of a particular layer. Take a look at [dropout in Tensorflow](https://www.tensorflow.org/api_docs/python/tf/layers/dropout). You can add it into your computation graph as a layer similarly to the Dense layer.</span> \n",
    "\n",
    "<span style=\"font-size:larger;\">However, there is one more thing you need to do before you can use it. Dropout should drop the activations only during training because we don't want to loose any information when we use the model. You will need to define a boolean Tensor that will tell the dropout layer if it's in the training or testing mode.</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3 (bonus points) ###\n",
    "\n",
    "<span style=\"font-size:larger;\">Add a dropout layer **before** the last Dense layer and make sure that activations are dropped only during training. Try changing the drop probability in order to obtain the best validation accuracy. Use the neural network you have defined above and train it with mini-batch gradient descent.</span>\n",
    "\n",
    "<span style=\"font-size:larger;\">The solution will be added to the reference notebook after the end of this tutorial.</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.reset_default_graph()   # TF remembers everything you defined, this will keep the computation graph clean\n",
    "\n",
    "learning_rate = 0.05\n",
    "rate = 0.55\n",
    "\n",
    "is_training = tf.placeholder(tf.bool)\n",
    "\n",
    "input_data = tf.placeholder(tf.float32, (None, train_dataset.shape[1]))\n",
    "input_labels = tf.placeholder(tf.int32, (None, train_labels.shape[1]))\n",
    "\n",
    "layer1 = tf.layers.dense(input_data / 255, 200, activation=tf.nn.relu)\n",
    "\n",
    "dropout1 = tf.layers.dropout(layer1, rate=rate, training=is_training)\n",
    "\n",
    "layer2 = tf.layers.dense(dropout1, 200, activation=tf.nn.relu)\n",
    "\n",
    "dropout2 = tf.layers.dropout(layer2, rate=rate, training=is_training)\n",
    "\n",
    "logits = tf.layers.dense(dropout2, 4)\n",
    "\n",
    "batch_loss = tf.nn.softmax_cross_entropy_with_logits(labels=input_labels, logits=logits)\n",
    "loss = tf.reduce_mean(batch_loss)\n",
    "train_op = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)\n",
    "\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(tf.argmax(logits, 1), tf.argmax(input_labels, 1)), tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 0 , loss: 1.6472831 , training accuracy: 0.28125\n",
      "step: 500 , loss: 1.0496225 , training accuracy: 0.515625\n",
      "step: 1000 , loss: 0.9597064 , training accuracy: 0.59375\n",
      "step: 1500 , loss: 1.2615576 , training accuracy: 0.421875\n",
      "step: 2000 , loss: 0.9666988 , training accuracy: 0.578125\n",
      "step: 2500 , loss: 1.1226506 , training accuracy: 0.5\n",
      "step: 3000 , loss: 1.0655395 , training accuracy: 0.453125\n",
      "step: 3500 , loss: 1.0706731 , training accuracy: 0.46875\n",
      "step: 4000 , loss: 1.0345829 , training accuracy: 0.515625\n",
      "step: 4500 , loss: 0.9784797 , training accuracy: 0.515625\n",
      "Training finished after 5000 steps.\n",
      "Validation accuracy 0.379 .\n"
     ]
    }
   ],
   "source": [
    "num_steps = 5000\n",
    "mini_batch_size = 64\n",
    "log_frequency = 500\n",
    "\n",
    "with tf.Session() as session:\n",
    "  session.run(tf.global_variables_initializer())\n",
    "\n",
    "  for step in range(num_steps):\n",
    "    start = step\n",
    "    end = step + mini_batch_size\n",
    "    \n",
    "    batch_loss, batch_accuracy, _ = session.run([loss, accuracy, train_op], feed_dict={\n",
    "      input_data: train_dataset.take(range(start, end), axis=0, mode=\"wrap\"),\n",
    "      input_labels: train_labels.take(range(start, end), axis=0, mode=\"wrap\"),\n",
    "      is_training: True\n",
    "    })\n",
    "    \n",
    "    if step % log_frequency == 0:\n",
    "      print('step:', step, ', loss:', batch_loss, ', training accuracy:', batch_accuracy)\n",
    "    \n",
    "  print('Training finished after', num_steps, 'steps.')\n",
    "  validation_accuracy = session.run(accuracy, feed_dict={\n",
    "    input_data: valid_dataset,\n",
    "    input_labels: valid_labels,\n",
    "    is_training: False\n",
    "  })\n",
    "  print('Validation accuracy', validation_accuracy, '.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Optional) Part 5: Convolutional Neural Networks ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-size:larger;\">Fully-connected neural network are not appropriate for modelling images becuase they aren't invariant to translations and have too many weights. For these reasons, a different type of neural network was developed. Convolutional Neural Networks (ConvNets) use filters and max-pooling layers to keep the number of weights low and to learn to recognize object regardless of their position in the image. Moreover, they are easy to implement in Tensorflow</span>\n",
    "\n",
    "<span style=\"font-size:larger;\">Implement a simple ConvNet with convolutional, max-pooling and dense layers.</span>\n",
    "\n",
    "<span style=\"font-size:larger;\">Useful links:</span>\n",
    "* [lecture notes Stanford University **(recommended)**](http://cs231n.github.io/convolutional-networks/)\n",
    "* [Tensorflow tutorial](https://www.tensorflow.org/tutorials/layers)\n",
    "\n",
    "<span style=\"font-size:larger;\">See the reference notebook for a solution.</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data_root' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-71-ef0422d6a492>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# load a subset of the notMNIST dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mpickle_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_root\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'notMNIST.pickle'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpickle_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0msave\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'data_root' is not defined"
     ]
    }
   ],
   "source": [
    "# load a subset of the notMNIST dataset\n",
    "pickle_file = os.path.join(data_root, 'notMNIST.pickle')\n",
    "\n",
    "with open(pickle_file, 'rb') as f:\n",
    "  save = pickle.load(f)\n",
    "  train_dataset = save['train_dataset']\n",
    "  train_labels = save['train_labels']\n",
    "  valid_dataset = save['valid_dataset']\n",
    "  valid_labels = save['valid_labels']\n",
    "  test_dataset = save['test_dataset']\n",
    "  test_labels = save['test_labels']\n",
    "  del save  # hint to help gc free up memory\n",
    "  print('Training set', train_dataset.shape, train_labels.shape)\n",
    "  print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "  print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training dataset shape: (8000, 32, 32, 3)\n",
      "Validation dataset shape: (1000, 32, 32, 3)\n",
      "Test dataset shape: (1000, 32, 32, 3)\n"
     ]
    }
   ],
   "source": [
    "def maybe_add_channels_dimension(dataset):\n",
    "  if len(dataset.shape) == 3:\n",
    "    return np.expand_dims(dataset, axis=-1)\n",
    "  else:\n",
    "    return dataset\n",
    "\n",
    "train_dataset = maybe_add_channels_dimension(train_dataset)\n",
    "valid_dataset = maybe_add_channels_dimension(valid_dataset)\n",
    "test_dataset = maybe_add_channels_dimension(test_dataset)\n",
    "\n",
    "print('Training dataset shape:', train_dataset.shape)\n",
    "print('Validation dataset shape:', valid_dataset.shape)\n",
    "print('Test dataset shape:', test_dataset.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training labels shape: (8000, 4)\n",
      "Validation labels shape: (1000, 4)\n",
      "Test labels shape: (1000, 4)\n"
     ]
    }
   ],
   "source": [
    "def maybe_turn_to_one_hot(labels, num_labels=4):\n",
    "  if len(labels.shape) == 1:\n",
    "    one_hot = np.zeros((labels.shape[0], num_labels))\n",
    "    one_hot[np.arange(len(labels)), labels] = 1\n",
    "    return one_hot\n",
    "  else:\n",
    "    return labels\n",
    "\n",
    "train_labels = maybe_turn_to_one_hot(train_labels)\n",
    "valid_labels = maybe_turn_to_one_hot(valid_labels)\n",
    "test_labels = maybe_turn_to_one_hot(test_labels)\n",
    "\n",
    "print('Training labels shape:', train_labels.shape)\n",
    "print('Validation labels shape:', valid_labels.shape)\n",
    "print('Test labels shape:', test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.reset_default_graph()   # TF remembers everything you defined, this will keep the computation graph clean\n",
    "\n",
    "learning_rate = 0.05\n",
    "\n",
    "input_data = tf.placeholder(tf.float32, (None, train_dataset.shape[1], train_dataset.shape[2], 3))\n",
    "input_labels = tf.placeholder(tf.int32, (None, train_labels.shape[1]))\n",
    "\n",
    "conv1 = tf.layers.conv2d(input_data / 255, 16, (3, 3), (1, 1), activation=tf.nn.relu)\n",
    "pool1 = tf.layers.max_pooling2d(conv1, (2, 2), (2, 2))\n",
    "\n",
    "conv2 = tf.layers.conv2d(pool1, 32, (3, 3), (1, 1), activation=tf.nn.relu)\n",
    "pool2 = tf.layers.max_pooling2d(conv2, (2, 2), (2, 2))\n",
    "\n",
    "flattened = tf.contrib.layers.flatten(pool2)\n",
    "\n",
    "dense1 = tf.layers.dense(flattened, 50)\n",
    "logits = tf.layers.dense(dense1, 4)\n",
    "\n",
    "batch_loss = tf.nn.softmax_cross_entropy_with_logits(labels=input_labels, logits=logits)\n",
    "loss = tf.reduce_mean(batch_loss)\n",
    "train_op = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)\n",
    "\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(tf.argmax(logits, 1), tf.argmax(input_labels, 1)), tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 0 , loss: 1.3556564 , training accuracy: 0.328125\n",
      "step: 100 , loss: 1.0885996 , training accuracy: 0.5\n",
      "step: 200 , loss: 0.6834685 , training accuracy: 0.828125\n",
      "step: 300 , loss: 0.5894709 , training accuracy: 0.8125\n",
      "step: 400 , loss: 0.15445615 , training accuracy: 0.96875\n",
      "step: 500 , loss: 0.896245 , training accuracy: 0.65625\n",
      "step: 600 , loss: 0.62140757 , training accuracy: 0.78125\n",
      "step: 700 , loss: 0.30756885 , training accuracy: 0.90625\n",
      "step: 800 , loss: 0.3614478 , training accuracy: 0.921875\n",
      "step: 900 , loss: 0.40963924 , training accuracy: 0.859375\n",
      "Training finished after 1000 steps.\n",
      "Validation accuracy 0.432 .\n"
     ]
    }
   ],
   "source": [
    "num_steps = 1000\n",
    "mini_batch_size = 64\n",
    "log_frequency = 100\n",
    "\n",
    "with tf.Session() as session:\n",
    "  session.run(tf.global_variables_initializer())\n",
    "\n",
    "  for step in range(num_steps):\n",
    "    start = step\n",
    "    end = step + mini_batch_size\n",
    "    \n",
    "    batch_loss, batch_accuracy, _ = session.run([loss, accuracy, train_op], feed_dict={\n",
    "      input_data: train_dataset.take(range(start, end), axis=0, mode=\"wrap\"),\n",
    "      input_labels: train_labels.take(range(start, end), axis=0, mode=\"wrap\")\n",
    "    })\n",
    "    \n",
    "    if step % log_frequency == 0:\n",
    "      print('step:', step, ', loss:', batch_loss, ', training accuracy:', batch_accuracy)\n",
    "    \n",
    "  print('Training finished after', num_steps, 'steps.')\n",
    "  validation_accuracy = session.run(accuracy, feed_dict={\n",
    "    input_data: valid_dataset,\n",
    "    input_labels: valid_labels\n",
    "  })\n",
    "  print('Validation accuracy', validation_accuracy, '.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Optional) Part 6: Regularizing ConvNets ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-size:larger;\">All neural networks are prone to overfitting if the training dataset is too small. Implement dropout for your ConvNet. See the reference notebook for a solution.</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.reset_default_graph()   # TF remembers everything you defined, this will keep the computation graph clean\n",
    "\n",
    "learning_rate = 0.05\n",
    "dropout_prob = 0.95\n",
    "\n",
    "input_data = tf.placeholder(tf.float32, (None, train_dataset.shape[1], train_dataset.shape[2], 3))\n",
    "input_labels = tf.placeholder(tf.int32, (None, train_labels.shape[1]))\n",
    "is_training = tf.placeholder(tf.bool)\n",
    "\n",
    "conv1 = tf.layers.conv2d(input_data / 255, 16, (3, 3), (1, 1), activation=tf.nn.relu)\n",
    "pool1 = tf.layers.max_pooling2d(conv1, (2, 2), (2, 2))\n",
    "\n",
    "conv2 = tf.layers.conv2d(pool1, 32, (3, 3), (1, 1), activation=tf.nn.relu)\n",
    "pool2 = tf.layers.max_pooling2d(conv2, (2, 2), (2, 2))\n",
    "\n",
    "flattened = tf.contrib.layers.flatten(pool2)\n",
    "\n",
    "dense1 = tf.layers.dense(flattened, 100)\n",
    "dropout = tf.layers.dropout(dense1, rate=dropout_prob, training=is_training)\n",
    "\n",
    "logits = tf.layers.dense(dropout, 4)\n",
    "\n",
    "batch_loss = tf.nn.softmax_cross_entropy_with_logits(labels=input_labels, logits=logits)\n",
    "loss = tf.reduce_mean(batch_loss)\n",
    "train_op = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)\n",
    "\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(tf.argmax(logits, 1), tf.argmax(input_labels, 1)), tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 0 , loss: 1.796901 , training accuracy: 0.234375\n",
      "step: 100 , loss: 1.3522882 , training accuracy: 0.296875\n",
      "step: 200 , loss: 1.2479856 , training accuracy: 0.390625\n",
      "step: 300 , loss: 1.2983861 , training accuracy: 0.40625\n",
      "step: 400 , loss: 1.2403922 , training accuracy: 0.46875\n",
      "step: 500 , loss: 1.2443997 , training accuracy: 0.421875\n",
      "step: 600 , loss: 1.1499411 , training accuracy: 0.5\n",
      "step: 700 , loss: 1.1049087 , training accuracy: 0.578125\n",
      "step: 800 , loss: 1.1692582 , training accuracy: 0.46875\n",
      "step: 900 , loss: 0.91896075 , training accuracy: 0.640625\n",
      "step: 1000 , loss: 1.0065439 , training accuracy: 0.5625\n",
      "step: 1100 , loss: 1.0984967 , training accuracy: 0.53125\n",
      "step: 1200 , loss: 0.94727653 , training accuracy: 0.640625\n",
      "step: 1300 , loss: 1.0428303 , training accuracy: 0.65625\n",
      "step: 1400 , loss: 1.0510213 , training accuracy: 0.625\n",
      "Training finished after 1500 steps.\n",
      "Validation accuracy 0.507 .\n"
     ]
    }
   ],
   "source": [
    "num_steps = 1500\n",
    "mini_batch_size = 64\n",
    "log_frequency = 100\n",
    "\n",
    "with tf.Session() as session:\n",
    "  session.run(tf.global_variables_initializer())\n",
    "\n",
    "  for step in range(num_steps):\n",
    "    start = step\n",
    "    end = step + mini_batch_size\n",
    "    \n",
    "    batch_loss, batch_accuracy, _ = session.run([loss, accuracy, train_op], feed_dict={\n",
    "      input_data: train_dataset.take(range(start, end), axis=0, mode=\"wrap\"),\n",
    "      input_labels: train_labels.take(range(start, end), axis=0, mode=\"wrap\"),\n",
    "      is_training: True\n",
    "    })\n",
    "    \n",
    "    if step % log_frequency == 0:\n",
    "      print('step:', step, ', loss:', batch_loss, ', training accuracy:', batch_accuracy)\n",
    "    \n",
    "  print('Training finished after', num_steps, 'steps.')\n",
    "  validation_accuracy = session.run(accuracy, feed_dict={\n",
    "    input_data: valid_dataset,\n",
    "    input_labels: valid_labels,\n",
    "    is_training: False\n",
    "  })\n",
    "  print('Validation accuracy', validation_accuracy, '.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Resources   ##\n",
    "\n",
    "** Saving and restoring models in Tensorfow **\n",
    "* [tutorial](https://www.tensorflow.org/programmers_guide/saved_model)\n",
    "\n",
    "** Visualizing learning using Tensorboard **\n",
    "* [tutorial](https://www.tensorflow.org/get_started/summaries_and_tensorboard)\n",
    "\n",
    "** Convolutional Networks **\n",
    "* [lecture notes Stanford University **(recommended)**](http://cs231n.github.io/convolutional-networks/)\n",
    "* [lecture video from the University of Oxford](https://www.youtube.com/watch?v=bEUX_56Lojc)\n",
    "* [Tensorflow tutorial](https://www.tensorflow.org/tutorials/layers)\n",
    "\n",
    "** Dropout **\n",
    "* [documentation](https://www.tensorflow.org/api_docs/python/tf/layers/dropout)\n",
    "* [paper](https://www.cs.toronto.edu/~hinton/absps/JMLRdropout.pdf)\n",
    "\n",
    "** Advanced data loading features in Tensorflow **\n",
    "* [tutorial](https://www.tensorflow.org/programmers_guide/datasets)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try out different dataset ##\n",
    "\n",
    "* [Dogs vs. Cats](https://www.kaggle.com/c/dogs-vs-cats)\n",
    "* [CIFAR-10](https://www.kaggle.com/c/cifar-10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
